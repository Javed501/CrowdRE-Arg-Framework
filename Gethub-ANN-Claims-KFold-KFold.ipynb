{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import keras\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "#nltk.download('popular')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from numpy import mean, std\n",
    "from sklearn.metrics import classification_report\n",
    "import plotly.graph_objs as go\n",
    "import plotly.plotly as py\n",
    "#import cufflinks\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import plotly.figure_factory as ff\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "from plotly.offline import iplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2077\n",
      "neu           772\n",
      "supporting    762\n",
      "attacking     542\n",
      "Name: Claim_Type, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_Text</th>\n",
       "      <th>comment_author</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_upvotes</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>Rationale_Type</th>\n",
       "      <th>Claim_Type</th>\n",
       "      <th>Have_Rationale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>People still do racial checkbox ads?</td>\n",
       "      <td>PubicLouseInDaHouse</td>\n",
       "      <td>dfadazi</td>\n",
       "      <td>-1</td>\n",
       "      <td>60v5za</td>\n",
       "      <td>claim</td>\n",
       "      <td>neu</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What?</td>\n",
       "      <td>Isvara</td>\n",
       "      <td>dfade5q</td>\n",
       "      <td>1</td>\n",
       "      <td>dfadazi</td>\n",
       "      <td>claim</td>\n",
       "      <td>neu</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90s power rangers.</td>\n",
       "      <td>PubicLouseInDaHouse</td>\n",
       "      <td>dfadlgi</td>\n",
       "      <td>1</td>\n",
       "      <td>dfade5q</td>\n",
       "      <td>claim</td>\n",
       "      <td>neu</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://support.google.com/plus/answer/2998354...</td>\n",
       "      <td>sparr</td>\n",
       "      <td>dfa365t</td>\n",
       "      <td>-1</td>\n",
       "      <td>60v5za</td>\n",
       "      <td>claim</td>\n",
       "      <td>supporting</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>This is really cool for people who have friends.</td>\n",
       "      <td>Unidan_nadinU</td>\n",
       "      <td>df9vhad</td>\n",
       "      <td>-1</td>\n",
       "      <td>60v5za</td>\n",
       "      <td>claim</td>\n",
       "      <td>supporting</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_Text       comment_author  \\\n",
       "1               People still do racial checkbox ads?  PubicLouseInDaHouse   \n",
       "2                                              What?               Isvara   \n",
       "3                                 90s power rangers.  PubicLouseInDaHouse   \n",
       "5  https://support.google.com/plus/answer/2998354...                sparr   \n",
       "6  This is really cool for people who have friends.         Unidan_nadinU   \n",
       "\n",
       "  comment_id  comment_upvotes parent_id Rationale_Type  Claim_Type  \\\n",
       "1    dfadazi               -1    60v5za          claim         neu   \n",
       "2    dfade5q                1   dfadazi          claim         neu   \n",
       "3    dfadlgi                1   dfade5q          claim         neu   \n",
       "5    dfa365t               -1    60v5za          claim  supporting   \n",
       "6    df9vhad               -1    60v5za          claim  supporting   \n",
       "\n",
       "  Have_Rationale  \n",
       "1             no  \n",
       "2             no  \n",
       "3             no  \n",
       "5            yes  \n",
       "6            yes  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('google-maps-single-dataset-for-conference.csv',encoding='ISO-8859-1')\n",
    "data= data[pd.notnull(data['Claim_Type'])]\n",
    "print(len(data))\n",
    "\n",
    "data = data.replace(\"neu \", \"neu\")\n",
    "data = data.replace(\"supporting \", \"supporting\")\n",
    "data = data.replace(\"attacking \", \"attacking\")\n",
    "data = data.replace(\"atacking \", \"attacking\")\n",
    "data = data.replace(\"attacknig \", \"attacking\")\n",
    "\n",
    "data= data[data.Rationale_Type=='claim']\n",
    "print(data.Claim_Type.value_counts())\n",
    "#data = data.replace(\"issue \", \"issue\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_plot(index):\n",
    "    example = data[data.index == index][['comment_Text', 'Rationale_Type']].values[0]\n",
    "    if len(example) > 0:\n",
    "        print(example[0])\n",
    "        print('Rationale_Type:', example[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> gender equality in any and all professions.\r\n",
      "\r\n",
      "Hahah uh... what?\r\n",
      "\r\n",
      "EDIT: Also I wasn't the one to downvote you above. No clue what that's about.\n",
      "Rationale_Type: claim\n"
     ]
    }
   ],
   "source": [
    "print_plot(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\<\\>\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z =#+_]')\n",
    "#STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_url(text):\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = strip_html(text) \n",
    "    text = remove_url(text)\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    #text = text.replace('x', '')\n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    #text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "data['comment_Text'] = data['comment_Text'].apply(clean_text)\n",
    "#df['Consumer complaint narrative'] = df['Consumer complaint narrative'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  gender equality in any and all professionshahah uh whatedit also i wasnt the one to downvote you above no clue what thats about\n",
      "Rationale_Type: claim\n"
     ]
    }
   ],
   "source": [
    "print_plot(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neu' 'supporting' 'attacking']\n"
     ]
    }
   ],
   "source": [
    "#print(data.Rationale_Type.value_counts())\n",
    "X= data.comment_Text.values.astype('U')\n",
    "y=data.Claim_Type.values.astype('U')\n",
    "rationale_type=data.Rationale_Type\n",
    "rationale_type_name= data.Claim_Type.unique()\n",
    "print(rationale_type_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size= 10000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "batch_size= 12\n",
    "epochs= 10\n",
    "test_accuracy =[]\n",
    "report_accuracy= []\n",
    "\n",
    "def evaluate_ANN_Model(trainX,trainy, testX, testy):\n",
    "    tokenizer= Tokenizer(num_words=vocab_size)\n",
    "    tokenizer.fit_on_texts(trainX)\n",
    "    X_train_matrix_tfidf= tokenizer.texts_to_matrix(trainX,mode='tfidf')\n",
    "    X_test_matrix_tfidf= tokenizer.texts_to_matrix(testX, mode='tfidf')\n",
    "\n",
    "    encoder= LabelBinarizer()\n",
    "    encoder.fit(trainy)\n",
    "    Y_train_matrix_tfidf= encoder.transform(trainy)\n",
    "    Y_test_matrix_tfidf= encoder.transform(testy)\n",
    "    smote = SMOTE()\n",
    "    X_sm_tfidf, y_sm_tfidf = smote.fit_sample(X_train_matrix_tfidf, Y_train_matrix_tfidf)\n",
    "    \n",
    "    #print(vocab_size)\n",
    "    model = Sequential()\n",
    "    #input layer\n",
    "    model.add(Dense(16, input_shape=(vocab_size, )))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "    #hidden layer\n",
    "    #odel.add(Dense(32))\n",
    "    #odel.add(Activation('relu'))\n",
    "    #odel.add(Dropout(0.3))\n",
    "    #output layer\n",
    "    model.add(Dense(3))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
    "    #model. summary()\n",
    "    #print(model.metrics_names)\n",
    "    #fit model \n",
    "    model.fit(X_sm_tfidf,y_sm_tfidf, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.2)\n",
    "    #evaluate model\n",
    "    score= model.evaluate(X_test_matrix_tfidf, Y_test_matrix_tfidf, batch_size=batch_size, verbose=1)\n",
    "    test_accuracy= score[1]\n",
    "    #predict model accuracy on testing data\n",
    "    pred = model.predict(X_test_matrix_tfidf, batch_size=12, verbose=1)\n",
    "    predicted = np.argmax(pred, axis=1)\n",
    "    report_accuracy = precision_recall_fscore_support(np.argmax(Y_test_matrix_tfidf, axis=1), predicted)\n",
    "    return model, test_accuracy, report_accuracy\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1341 samples, validate on 336 samples\n",
      "Epoch 1/10\n",
      "1341/1341 [==============================] - 17s 12ms/step - loss: 1.1167 - acc: 0.3594 - val_loss: 1.0392 - val_acc: 0.4464\n",
      "Epoch 2/10\n",
      "1341/1341 [==============================] - 1s 1ms/step - loss: 0.9846 - acc: 0.4884 - val_loss: 0.9592 - val_acc: 0.5536\n",
      "Epoch 3/10\n",
      "1341/1341 [==============================] - 1s 800us/step - loss: 0.8792 - acc: 0.5839 - val_loss: 0.8727 - val_acc: 0.6458\n",
      "Epoch 4/10\n",
      "1341/1341 [==============================] - 1s 668us/step - loss: 0.7733 - acc: 0.6339 - val_loss: 0.7852 - val_acc: 0.6964\n",
      "Epoch 5/10\n",
      "1341/1341 [==============================] - 1s 625us/step - loss: 0.6760 - acc: 0.6980 - val_loss: 0.6838 - val_acc: 0.7649\n",
      "Epoch 6/10\n",
      "1341/1341 [==============================] - 1s 645us/step - loss: 0.5708 - acc: 0.7547 - val_loss: 0.5849 - val_acc: 0.8155\n",
      "Epoch 7/10\n",
      "1341/1341 [==============================] - 1s 593us/step - loss: 0.5001 - acc: 0.7934 - val_loss: 0.5296 - val_acc: 0.8423\n",
      "Epoch 8/10\n",
      "1341/1341 [==============================] - 1s 623us/step - loss: 0.4468 - acc: 0.8084 - val_loss: 0.4832 - val_acc: 0.8601\n",
      "Epoch 9/10\n",
      "1341/1341 [==============================] - 1s 591us/step - loss: 0.4012 - acc: 0.8285 - val_loss: 0.4445 - val_acc: 0.8780\n",
      "Epoch 10/10\n",
      "1341/1341 [==============================] - 1s 612us/step - loss: 0.3751 - acc: 0.8292 - val_loss: 0.4179 - val_acc: 0.8690\n",
      "692/692 [==============================] - 0s 289us/step\n",
      "692/692 [==============================] - 1s 2ms/step\n",
      "the training accuracy is 0.48265895781489465\n",
      "the testing accuracy is (array([0.39735099, 0.44615385, 0.56227758]), array([0.32085561, 0.54460094, 0.54109589]), array([0.35502959, 0.49048626, 0.55148342]), array([187, 213, 292], dtype=int64))\n",
      "Train on 1264 samples, validate on 317 samples\n",
      "Epoch 1/10\n",
      "1264/1264 [==============================] - 10s 8ms/step - loss: 1.1079 - acc: 0.4043 - val_loss: 1.0817 - val_acc: 0.2871\n",
      "Epoch 2/10\n",
      "1264/1264 [==============================] - 1s 1ms/step - loss: 0.9699 - acc: 0.5222 - val_loss: 1.0374 - val_acc: 0.4101\n",
      "Epoch 3/10\n",
      "1264/1264 [==============================] - 1s 854us/step - loss: 0.8395 - acc: 0.6084 - val_loss: 1.0038 - val_acc: 0.5205\n",
      "Epoch 4/10\n",
      "1264/1264 [==============================] - 1s 741us/step - loss: 0.7172 - acc: 0.6804 - val_loss: 0.9120 - val_acc: 0.6656\n",
      "Epoch 5/10\n",
      "1264/1264 [==============================] - 1s 620us/step - loss: 0.6270 - acc: 0.7231 - val_loss: 0.9010 - val_acc: 0.6909\n",
      "Epoch 6/10\n",
      "1264/1264 [==============================] - 1s 618us/step - loss: 0.5563 - acc: 0.7611 - val_loss: 0.8250 - val_acc: 0.7319\n",
      "Epoch 7/10\n",
      "1264/1264 [==============================] - 1s 645us/step - loss: 0.4881 - acc: 0.7896 - val_loss: 0.7865 - val_acc: 0.7603\n",
      "Epoch 8/10\n",
      "1264/1264 [==============================] - 1s 665us/step - loss: 0.4471 - acc: 0.8054 - val_loss: 0.7617 - val_acc: 0.7729\n",
      "Epoch 9/10\n",
      "1264/1264 [==============================] - 1s 649us/step - loss: 0.3940 - acc: 0.8465 - val_loss: 0.7602 - val_acc: 0.7792\n",
      "Epoch 10/10\n",
      "1264/1264 [==============================] - 1s 639us/step - loss: 0.3585 - acc: 0.8386 - val_loss: 0.7430 - val_acc: 0.7697\n",
      "692/692 [==============================] - 0s 308us/step\n",
      "692/692 [==============================] - 2s 3ms/step\n",
      "the training accuracy is 0.4335260088905434\n",
      "the testing accuracy is (array([0.38418079, 0.44628099, 0.45421245]), array([0.34871795, 0.44081633, 0.49206349]), array([0.3655914 , 0.44353183, 0.47238095]), array([195, 245, 252], dtype=int64))\n",
      "Train on 1305 samples, validate on 327 samples\n",
      "Epoch 1/10\n",
      "1305/1305 [==============================] - 8s 6ms/step - loss: 1.0997 - acc: 0.3816 - val_loss: 1.0731 - val_acc: 0.3731\n",
      "Epoch 2/10\n",
      "1305/1305 [==============================] - 2s 2ms/step - loss: 0.9856 - acc: 0.5088 - val_loss: 1.0196 - val_acc: 0.4924c: 0.50\n",
      "Epoch 3/10\n",
      "1305/1305 [==============================] - 1s 1ms/step - loss: 0.8653 - acc: 0.6061 - val_loss: 0.9678 - val_acc: 0.5719\n",
      "Epoch 4/10\n",
      "1305/1305 [==============================] - 1s 816us/step - loss: 0.7528 - acc: 0.6927 - val_loss: 0.8807 - val_acc: 0.6911\n",
      "Epoch 5/10\n",
      "1305/1305 [==============================] - 1s 709us/step - loss: 0.6444 - acc: 0.7257 - val_loss: 0.8395 - val_acc: 0.7156\n",
      "Epoch 6/10\n",
      "1305/1305 [==============================] - 1s 640us/step - loss: 0.5769 - acc: 0.7632 - val_loss: 0.7705 - val_acc: 0.7401\n",
      "Epoch 7/10\n",
      "1305/1305 [==============================] - 1s 635us/step - loss: 0.5092 - acc: 0.7939 - val_loss: 0.7344 - val_acc: 0.7523\n",
      "Epoch 8/10\n",
      "1305/1305 [==============================] - 1s 653us/step - loss: 0.4870 - acc: 0.7992 - val_loss: 0.6870 - val_acc: 0.7829\n",
      "Epoch 9/10\n",
      "1305/1305 [==============================] - 1s 631us/step - loss: 0.4144 - acc: 0.8138 - val_loss: 0.6864 - val_acc: 0.7768\n",
      "Epoch 10/10\n",
      "1305/1305 [==============================] - 1s 623us/step - loss: 0.3746 - acc: 0.8444 - val_loss: 0.6544 - val_acc: 0.8073\n",
      "692/692 [==============================] - 0s 283us/step\n",
      "692/692 [==============================] - 1s 1ms/step\n",
      "the training accuracy is 0.48843930523864104\n",
      "the testing accuracy is (array([0.36787565, 0.65044248, 0.43956044]), array([0.44375   , 0.46815287, 0.55045872]), array([0.40226629, 0.54444444, 0.48879837]), array([160, 314, 218], dtype=int64))\n",
      "the average Training Accuracy of ANN-TFIDF is 0.4682080906480264\n",
      "the averag Validation accuracy of ANN-TFIDF is [[  0.38313581   0.51429244   0.48535016]\n",
      " [  0.37110785   0.48452338   0.5278727 ]\n",
      " [  0.37429576   0.49282084   0.50422091]\n",
      " [180.66666667 257.33333333 254.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Kfold ANN with TFIDF\n",
    "\n",
    "kf= KFold(n_splits=3)\n",
    "#curr_fold= 0\n",
    "alg_accurcy_ANN_tfidf =[]\n",
    "alg_testing_accurcy_ANN_tfidf =[]\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test= X[train_idx], X[test_idx]\n",
    "    y_train, y_test= y[train_idx], y[test_idx]\n",
    "        \n",
    "    model, test_accuracy,report_accuracy = evaluate_ANN_Model(X_train,y_train, X_test, y_test )\n",
    "    print (\"the training accuracy is\",test_accuracy )\n",
    "    alg_accurcy_ANN_tfidf.append(test_accuracy)\n",
    "    print (\"the testing accuracy is\",report_accuracy )\n",
    "    alg_testing_accurcy_ANN_tfidf.append(report_accuracy)\n",
    "    \n",
    "\n",
    "average_accuracy_ANN_training= np.mean(alg_accurcy_ANN_tfidf, axis=0)\n",
    "print(\"the average Training Accuracy of ANN-TFIDF is\",average_accuracy_ANN_training) \n",
    "\n",
    "average_accuracy_ANN_testing= np.mean(alg_testing_accurcy_ANN_tfidf, axis=0)\n",
    "print(\"the averag Validation accuracy of ANN-TFIDF is\",average_accuracy_ANN_testing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size= 10000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "batch_size= 12\n",
    "epochs= 10\n",
    "test_accuracy =[]\n",
    "report_accuracy= []\n",
    "\n",
    "def evaluate_ANN_Model(trainX,trainy, testX, testy):\n",
    "    \n",
    "    \n",
    "    tokenizer= Tokenizer(num_words=vocab_size)\n",
    "    tokenizer.fit_on_texts(trainX)\n",
    "    X_train_matrix_binary= tokenizer.texts_to_matrix(trainX,mode='binary')\n",
    "    X_test_matrix_binary= tokenizer.texts_to_matrix(testX, mode='binary')\n",
    "\n",
    "    encoder= LabelBinarizer()\n",
    "    encoder.fit(trainy)\n",
    "    Y_train_matrix_binary= encoder.transform(trainy)\n",
    "    Y_test_matrix_binary= encoder.transform(testy)\n",
    "    \n",
    "    \n",
    "    smote = SMOTE()\n",
    "    X_sm_binary, y_sm_binary = smote.fit_sample(X_train_matrix_binary, Y_train_matrix_binary)\n",
    "    \n",
    "    MAX_SEQUENCE_LENGTH = 1000\n",
    "    #print(vocab_size)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_shape=(vocab_size, )))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "    #hidden layer\n",
    "    #model.add(Dense(16))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(3))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
    "    #model. summary()\n",
    "    #print(model.metrics_names)\n",
    "\n",
    "    model.fit(X_sm_binary,y_sm_binary, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.2)\n",
    "    score= model.evaluate(X_test_matrix_binary, Y_test_matrix_binary, batch_size=batch_size, verbose=1)\n",
    "    test_accuracy= score[1]\n",
    "    #predict model accuracy on testing data\n",
    "    pred = model.predict(X_test_matrix_binary, batch_size=12, verbose=1)\n",
    "    predicted = np.argmax(pred, axis=1)\n",
    "    report_accuracy = precision_recall_fscore_support(np.argmax(Y_test_matrix_binary, axis=1), predicted)\n",
    "    return model, test_accuracy, report_accuracy  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1010 samples, validate on 253 samples\n",
      "Epoch 1/10\n",
      "1010/1010 [==============================] - 5s 5ms/step - loss: 1.0960 - acc: 0.4069 - val_loss: 1.0753 - val_acc: 0.4704: 1s - loss: 1.1003 - acc\n",
      "Epoch 2/10\n",
      "1010/1010 [==============================] - 2s 2ms/step - loss: 1.0294 - acc: 0.5287 - val_loss: 1.0584 - val_acc: 0.4032\n",
      "Epoch 3/10\n",
      "1010/1010 [==============================] - 2s 2ms/step - loss: 0.9470 - acc: 0.5980 - val_loss: 0.9904 - val_acc: 0.5573\n",
      "Epoch 4/10\n",
      "1010/1010 [==============================] - 1s 1ms/step - loss: 0.8424 - acc: 0.6713 - val_loss: 0.9372 - val_acc: 0.5771\n",
      "Epoch 5/10\n",
      "1010/1010 [==============================] - 1s 787us/step - loss: 0.7418 - acc: 0.7198 - val_loss: 0.8872 - val_acc: 0.6206\n",
      "Epoch 6/10\n",
      "1010/1010 [==============================] - 1s 726us/step - loss: 0.6688 - acc: 0.7505 - val_loss: 0.7919 - val_acc: 0.6996\n",
      "Epoch 7/10\n",
      "1010/1010 [==============================] - 1s 631us/step - loss: 0.5938 - acc: 0.7950 - val_loss: 0.7407 - val_acc: 0.7273\n",
      "Epoch 8/10\n",
      "1010/1010 [==============================] - 1s 615us/step - loss: 0.5406 - acc: 0.8158 - val_loss: 0.7010 - val_acc: 0.7431\n",
      "Epoch 9/10\n",
      "1010/1010 [==============================] - 1s 598us/step - loss: 0.4827 - acc: 0.8446 - val_loss: 0.6586 - val_acc: 0.7628\n",
      "Epoch 10/10\n",
      "1010/1010 [==============================] - 1s 581us/step - loss: 0.4593 - acc: 0.8406 - val_loss: 0.6110 - val_acc: 0.8024\n",
      "1038/1038 [==============================] - 0s 261us/step\n",
      "1038/1038 [==============================] - ETA:  - 1s 1ms/step\n",
      "the training accuracy is 0.4499036607714747\n",
      "the testing accuracy is (array([0.34538153, 0.43908046, 0.53672316]), array([0.32089552, 0.54415954, 0.45346062]), array([0.33268859, 0.48600509, 0.4915912 ]), array([268, 351, 419], dtype=int64))\n",
      "Train on 1005 samples, validate on 252 samples\n",
      "Epoch 1/10\n",
      "1005/1005 [==============================] - 7s 7ms/step - loss: 1.0944 - acc: 0.3821 - val_loss: 1.1064 - val_acc: 0.1825\n",
      "Epoch 2/10\n",
      "1005/1005 [==============================] - 2s 2ms/step - loss: 1.0505 - acc: 0.4697 - val_loss: 1.0999 - val_acc: 0.3095\n",
      "Epoch 3/10\n",
      "1005/1005 [==============================] - 1s 1ms/step - loss: 0.9842 - acc: 0.5473 - val_loss: 1.0471 - val_acc: 0.4881\n",
      "Epoch 4/10\n",
      "1005/1005 [==============================] - 1s 984us/step - loss: 0.8942 - acc: 0.6249 - val_loss: 0.9992 - val_acc: 0.5635\n",
      "Epoch 5/10\n",
      "1005/1005 [==============================] - 1s 806us/step - loss: 0.8085 - acc: 0.6736 - val_loss: 0.9305 - val_acc: 0.6429\n",
      "Epoch 6/10\n",
      "1005/1005 [==============================] - 1s 695us/step - loss: 0.7294 - acc: 0.7264 - val_loss: 0.8731 - val_acc: 0.6627\n",
      "Epoch 7/10\n",
      "1005/1005 [==============================] - 1s 631us/step - loss: 0.6673 - acc: 0.7483 - val_loss: 0.8538 - val_acc: 0.6508\n",
      "Epoch 8/10\n",
      "1005/1005 [==============================] - 1s 561us/step - loss: 0.6071 - acc: 0.7821 - val_loss: 0.8049 - val_acc: 0.7143\n",
      "Epoch 9/10\n",
      "1005/1005 [==============================] - 1s 596us/step - loss: 0.5437 - acc: 0.8229 - val_loss: 0.7791 - val_acc: 0.7063\n",
      "Epoch 10/10\n",
      "1005/1005 [==============================] - 1s 524us/step - loss: 0.5007 - acc: 0.8318 - val_loss: 0.7362 - val_acc: 0.7460\n",
      "1038/1038 [==============================] - 0s 251us/step\n",
      "1038/1038 [==============================] - 1s 1ms/step\n",
      "the training accuracy is 0.4971098237471773\n",
      "the testing accuracy is (array([0.40611354, 0.55919395, 0.48786408]), array([0.33941606, 0.52731591, 0.58600583]), array([0.36978131, 0.54278729, 0.53245033]), array([274, 421, 343], dtype=int64))\n",
      "the average Training Accuracy of ANN-Binary is 0.4754335246764856\n",
      "the averag Validation accuracy of ANN-Binary is [[  0.38392176   0.50930481   0.50300061]\n",
      " [  0.35995732   0.48316283   0.55748624]\n",
      " [  0.36989513   0.49077718   0.52700526]\n",
      " [180.66666667 257.33333333 254.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Kfold ANN with mode=binary\n",
    "\n",
    "kf= KFold(n_splits=2)\n",
    "#curr_fold= 0\n",
    "alg_accurcy_ANN_binary =[]\n",
    "alg_testing_accurcy_ANN_binary =[]\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test= X[train_idx], X[test_idx]\n",
    "    y_train, y_test= y[train_idx], y[test_idx]\n",
    "        \n",
    "    model, test_accuracy,report_accuracy = evaluate_ANN_Model(X_train,y_train, X_test, y_test )\n",
    "    print (\"the training accuracy is\",test_accuracy )\n",
    "    alg_accurcy_ANN_binary.append(test_accuracy)\n",
    "    print (\"the testing accuracy is\",report_accuracy )\n",
    "    alg_testing_accurcy_ANN_binary.append(report_accuracy)\n",
    "    \n",
    "\n",
    "average_accuracy_ANN_binary= np.mean(alg_accurcy_ANN_tfidf, axis=0)\n",
    "print(\"the average Training Accuracy of ANN-Binary is\",average_accuracy_ANN_binary) \n",
    "\n",
    "average_accuracy_ANN_binary= np.mean(alg_testing_accurcy_ANN_tfidf, axis=0)\n",
    "print(\"the averag Validation accuracy of ANN-Binary is\",average_accuracy_ANN_binary)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size= 10000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "batch_size= 12\n",
    "epochs= 10\n",
    "test_accuracy =[]\n",
    "report_accuracy= []\n",
    "\n",
    "def evaluate_ANN_Model(trainX,trainy, testX, testy):\n",
    "    \n",
    "    tokenizer= Tokenizer(num_words=vocab_size)\n",
    "    tokenizer.fit_on_texts(trainX)\n",
    "    X_train_matrix_count= tokenizer.texts_to_matrix(trainX,mode='count')\n",
    "    X_test_matrix_count= tokenizer.texts_to_matrix(testX, mode='count')\n",
    "\n",
    "    encoder= LabelBinarizer()\n",
    "    encoder.fit(trainy)\n",
    "    Y_train_matrix_count= encoder.transform(trainy)\n",
    "    Y_test_matrix_count= encoder.transform(testy)     \n",
    "    \n",
    "    smote = SMOTE('minority')\n",
    "    X_sm_count, y_sm_count = smote.fit_sample(X_train_matrix_count, Y_train_matrix_count)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_shape=(vocab_size, )))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(3))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
    "    #model. summary()\n",
    "    #print(model.metrics_names)\n",
    "    from sklearn.utils import class_weight\n",
    "    class_weight= class_weight.compute_class_weight('balanced', np.unique(rationale_type),rationale_type)\n",
    "    #print(class_weight)\n",
    "    model.fit(X_sm_count,y_sm_count, batch_size=batch_size, class_weight=class_weight, epochs=epochs, verbose=1, validation_split=0.3)\n",
    "    score=model.evaluate(X_test_matrix_count, Y_test_matrix_count, batch_size=batch_size, verbose=1)\n",
    "    test_accuracy= score[1]\n",
    "    #predict model accuracy on testing data\n",
    "    pred = model.predict(X_test_matrix_count, batch_size=12, verbose=1)\n",
    "    predicted = np.argmax(pred, axis=1)\n",
    "    report_accuracy = precision_recall_fscore_support(np.argmax(Y_test_matrix_count, axis=1), predicted)\n",
    "    return model, test_accuracy, report_accuracy  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 829 samples, validate on 356 samples\n",
      "Epoch 1/10\n",
      "829/829 [==============================] - 8s 10ms/step - loss: 1.0949 - acc: 0.3776 - val_loss: 1.0875 - val_acc: 0.3933\n",
      "Epoch 2/10\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 1.0282 - acc: 0.5476 - val_loss: 1.0540 - val_acc: 0.4354\n",
      "Epoch 3/10\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.9031 - acc: 0.6779 - val_loss: 1.0102 - val_acc: 0.4944\n",
      "Epoch 4/10\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.7941 - acc: 0.7358 - val_loss: 1.0095 - val_acc: 0.4860\n",
      "Epoch 5/10\n",
      "829/829 [==============================] - 1s 892us/step - loss: 0.6576 - acc: 0.7973 - val_loss: 0.9792 - val_acc: 0.5421\n",
      "Epoch 6/10\n",
      "829/829 [==============================] - 1s 774us/step - loss: 0.5833 - acc: 0.8287 - val_loss: 0.9451 - val_acc: 0.5674\n",
      "Epoch 7/10\n",
      "829/829 [==============================] - 1s 719us/step - loss: 0.4724 - acc: 0.8842 - val_loss: 0.9382 - val_acc: 0.5787\n",
      "Epoch 8/10\n",
      "829/829 [==============================] - 1s 666us/step - loss: 0.4146 - acc: 0.9083 - val_loss: 0.9555 - val_acc: 0.5815\n",
      "Epoch 9/10\n",
      "829/829 [==============================] - 1s 645us/step - loss: 0.3649 - acc: 0.9131 - val_loss: 0.9477 - val_acc: 0.6208\n",
      "Epoch 10/10\n",
      "829/829 [==============================] - 1s 604us/step - loss: 0.3195 - acc: 0.9288 - val_loss: 0.9592 - val_acc: 0.6152\n",
      "1038/1038 [==============================] - 0s 277us/step\n",
      "1038/1038 [==============================] - 1s 1ms/step\n",
      "the training accuracy is 0.45664739539857546\n",
      "the testing accuracy is (array([0.34821429, 0.45      , 0.52941176]), array([0.29104478, 0.56410256, 0.4725537 ]), array([0.31707317, 0.50063211, 0.49936948]), array([268, 351, 419], dtype=int64))\n",
      "Train on 832 samples, validate on 357 samples\n",
      "Epoch 1/10\n",
      "832/832 [==============================] - 6s 7ms/step - loss: 1.0913 - acc: 0.4014 - val_loss: 1.0881 - val_acc: 0.2577\n",
      "Epoch 2/10\n",
      "832/832 [==============================] - 1s 2ms/step - loss: 0.9996 - acc: 0.5661 - val_loss: 1.0932 - val_acc: 0.2941\n",
      "Epoch 3/10\n",
      "832/832 [==============================] - 1s 1ms/step - loss: 0.8905 - acc: 0.6611 - val_loss: 1.0414 - val_acc: 0.4258\n",
      "Epoch 4/10\n",
      "832/832 [==============================] - 1s 1ms/step - loss: 0.7723 - acc: 0.7596 - val_loss: 1.0391 - val_acc: 0.4286\n",
      "Epoch 5/10\n",
      "832/832 [==============================] - 1s 900us/step - loss: 0.6504 - acc: 0.8089 - val_loss: 1.0337 - val_acc: 0.4650\n",
      "Epoch 6/10\n",
      "832/832 [==============================] - 1s 794us/step - loss: 0.5690 - acc: 0.8365 - val_loss: 1.0094 - val_acc: 0.4902\n",
      "Epoch 7/10\n",
      "832/832 [==============================] - 1s 717us/step - loss: 0.4933 - acc: 0.8738 - val_loss: 1.0067 - val_acc: 0.5126\n",
      "Epoch 8/10\n",
      "832/832 [==============================] - 1s 671us/step - loss: 0.4251 - acc: 0.9014 - val_loss: 1.0299 - val_acc: 0.5098\n",
      "Epoch 9/10\n",
      "832/832 [==============================] - 1s 644us/step - loss: 0.3803 - acc: 0.9099 - val_loss: 1.0368 - val_acc: 0.5238\n",
      "Epoch 10/10\n",
      "832/832 [==============================] - 1s 611us/step - loss: 0.3268 - acc: 0.9159 - val_loss: 1.0481 - val_acc: 0.5350\n",
      "1038/1038 [==============================] - 0s 268us/step\n",
      "1038/1038 [==============================] - 2s 2ms/step\n",
      "the training accuracy is 0.4633911369163866\n",
      "the testing accuracy is (array([0.36220472, 0.5659824 , 0.44243792]), array([0.33576642, 0.4584323 , 0.57142857]), array([0.34848485, 0.50656168, 0.49872774]), array([274, 421, 343], dtype=int64))\n",
      "the average Training Accuracy of ANN-Binary is 0.460019266157481\n",
      "the averag Validation accuracy of ANN-Binary is [[3.55209505e-01 5.07991202e-01 4.85924844e-01]\n",
      " [3.13405600e-01 5.11267434e-01 5.21991135e-01]\n",
      " [3.32779010e-01 5.03596896e-01 4.99048609e-01]\n",
      " [2.71000000e+02 3.86000000e+02 3.81000000e+02]]\n"
     ]
    }
   ],
   "source": [
    "#Kfold ANN with mode=count\n",
    "\n",
    "kf= KFold(n_splits=2)\n",
    "#curr_fold= 0\n",
    "alg_accurcy_ANN_count =[]\n",
    "alg_testing_accurcy_ANN_count =[]\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test= X[train_idx], X[test_idx]\n",
    "    y_train, y_test= y[train_idx], y[test_idx]\n",
    "        \n",
    "    model, test_accuracy,report_accuracy = evaluate_ANN_Model(X_train,y_train, X_test, y_test )\n",
    "    print (\"the training accuracy is\",test_accuracy )\n",
    "    alg_accurcy_ANN_count.append(test_accuracy)\n",
    "    print (\"the testing accuracy is\",report_accuracy )\n",
    "    alg_testing_accurcy_ANN_count.append(report_accuracy)\n",
    "    \n",
    "\n",
    "average_accuracy_ANN_count= np.mean(alg_accurcy_ANN_count, axis=0)\n",
    "print(\"the average Training Accuracy of ANN-Binary is\",average_accuracy_ANN_count) \n",
    "\n",
    "average_accuracy_ANN_count= np.mean(alg_testing_accurcy_ANN_count, axis=0)\n",
    "print(\"the averag Validation accuracy of ANN-Binary is\",average_accuracy_ANN_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
