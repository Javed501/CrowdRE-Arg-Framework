{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('Google Maps is testing a combined Commute tab to replace Driving and Transit.csv')\n",
    "#df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_sets = {}\n",
    "same_sets2 = {}\n",
    "comment_ids = df[\"comment_id\"].values.tolist()\n",
    "parent_ids = df[\"parent_id\"].values.tolist()\n",
    "rationale_types = df[\"Rationale_Type\"].values.tolist()\n",
    "\n",
    "for i in range(len(comment_ids)):\n",
    "    if rationale_types[i] == 'claim-support':\n",
    "        same_sets[comment_ids[i]] = parent_ids[i]\n",
    "        if parent_ids[i] not in same_sets2:\n",
    "            same_sets2[parent_ids[i]] = [comment_ids[i]]\n",
    "        else:\n",
    "            same_sets2[parent_ids[i]].append(comment_ids[i])\n",
    "                                        \n",
    "for i in range(len(comment_ids)):\n",
    "    parent_id = parent_ids[i]\n",
    "    while(parent_id in same_sets):\n",
    "        parent_id = same_sets[parent_id]\n",
    "    parent_ids[i] = parent_id\n",
    "    \n",
    "new_df = pd.DataFrame({'comment_id': comment_ids, 'parent_id': parent_ids, 'Rationale_Type': rationale_types})\n",
    "df = new_df[new_df[\"Rationale_Type\"] != \"claim_support\"]\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'9clgh7': ['e5bqkhk', 'e5bwn4g', 'e5btxu4', 'e5bmpya', 'e5bs4w3', 'e5c9mr9', 'e5bwzrb', 'e5d5dph', 'e5d3fyg', 'e5d237r', 'e5colkv', 'e5bumvv', 'e5bhqef', 'e5c1e4u', 'e5bs6c7', 'e5bhr9p', 'e5crv0y', 'e5bpnup', 'e5bi1ce', 'e5bh1by'], 'e5bqkhk': ['e5c4xqn'], 'e5bs6c7': ['e5c7vu0', 'e5d41nl', 'e5c08ws', 'e5c3xex'], 'e5c7vu0': ['e5d85sh', 'e5cgv1o'], 'e5bhr9p': ['e5choob', 'e5bixvt'], 'e5bixvt': ['e5bqzlk', 'e5bo31c', 'e5bqj1h'], 'e5bqzlk': ['e5ciyid'], 'e5bo31c': ['e5bv6i3', 'e5bwgvp'], 'e5crv0y': ['e5cv318'], 'e5bpnup': ['e5bu127', 'e5d9oa2', 'e5c2txt'], 'e5c2txt': ['e5chm3o'], 'e5bi1ce': ['e5bs0cs', 'e5c03j5', 'e5bu0gl', 'e5bo4qs'], 'e5bs0cs': ['e5c0qe6', 'e5c574t', 'e5d7mqx', 'e5d9jcp'], 'e5d9jcp': ['e5dizid'], 'e5bo4qs': ['e5bqf79', 'e5bv2j3'], 'e5bh1by': ['e5bhwu7', 'e5c001z', 'e5bk8f0', 'e5bhsdu'], 'e5c001z': ['e5chi75', 'e5d0jfz'], 'e5d0jfz': ['e5daofw'], 'e5daofw': ['e5dbmuv'], 'e5bk8f0': ['e5bu4b5'], 'e5bu4b5': ['e5diuxz'], 'e5bhsdu': ['e5bzx5i', 'e5bo0t5'], 'e5bzx5i': ['e5ctlzb'], 'e5ctlzb': ['e5djgug', 'e5cyx3h', 'e5cwhu1'], 'e5cyx3h': ['e5czfpz'], 'e5czfpz': ['e5dbn4z', 'e5d6vd7'], 'e5bo0t5': ['e5c9tbc']}\n"
     ]
    }
   ],
   "source": [
    "# df= pd.read_csv('example-case1.csv')\n",
    "\n",
    "########function#####\n",
    "\n",
    "def create_dictionary(key,values):\n",
    "    node_edges_list= {}  \n",
    "    #print(values)\n",
    "    for x in key:\n",
    "        node_edges_list[x]=values  \n",
    "    return(node_edges_list)\n",
    "\n",
    "######end-funtion####\n",
    "# nodes= df['comment_Text']\n",
    "lista= [] #create empty list\n",
    "node_edges= {} # empty dictionary for making nodes-edges pairs\n",
    "final_node_edges_dic={}\n",
    "#edges1= ['0','9clgh7', 'e5bqkhk']\n",
    "edges1= df['parent_id'].unique() # get unique colums values\n",
    "remove_top_element=list(edges1)  # convert array into list\n",
    "remove_top_element.pop(0) # remove top element from list which is root node\n",
    "#print(remove_top_element)\n",
    "#print(edges1)\n",
    "for edge in range(len(remove_top_element)): # code for making pairs of edges and nodes from the dataset\n",
    "    #print(edges1[edge])\n",
    "    lista.append(remove_top_element[edge])\n",
    "    #print(\"parent_id\",lista)\n",
    "    edge_single=df.loc[df['parent_id'].isin(lista)] #get comments based on primary key and forign key\n",
    "    edges_vertices=edge_single['comment_id'].values.tolist() # get correspoding comment_id for each vertices, which will shows possible reation between 2 vertices\n",
    "    #edges_vertices_list.append(edges_vertices) #getting values from dataform and convert that into list.\n",
    "    #print(\"comments_id\",edges_vertices)\n",
    "    node_edges= create_dictionary(lista,edges_vertices)\n",
    "    final_node_edges_dic.update(node_edges) \n",
    "    \n",
    "    lista =[]\n",
    "#graph= {1:[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], 2:[21], 21: [], 3:[]}\n",
    "print(final_node_edges_dic) #final list of pairs\n",
    "#visited = dfs(final_node_edges_dic,'9clgh7')\n",
    "#print(visited)\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "father_children = final_node_edges_dic\n",
    "node_height = {}\n",
    "for father in father_children:\n",
    "    cur_height = 0\n",
    "    if father in node_height:\n",
    "        cur_height = node_height[father]\n",
    "    else:\n",
    "        node_height[father] = 0\n",
    "    children = father_children[father]\n",
    "    for child in children:\n",
    "        if child not in node_height:\n",
    "            node_height[child] = cur_height + 1\n",
    "node_height_sorted = sorted(node_height.items(), key=lambda d:d[1], reverse=False)\n",
    "O = [x[0] for x in node_height_sorted]\n",
    "\n",
    "children_father = {}\n",
    "for father in father_children:\n",
    "    children = father_children[father]\n",
    "    for child in children:\n",
    "        children_father[child] = father\n",
    "\n",
    "comment_ids = df[\"comment_id\"]\n",
    "tags = df[\"Rationale_Type\"]\n",
    "tags = tags.fillna(0)\n",
    "id_tag = {}\n",
    "for index, comment_id in comment_ids.items():\n",
    "    id_tag[comment_id] = tags[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reverse(O): \n",
    "    return [ele for ele in reversed(O)] \n",
    "      \n",
    "# Driver Code \n",
    "#lst = [10, 11, 12, 13, 14, 15]\n",
    "reverse_O=Reverse(O)\n",
    "#print(reverse_O) \n",
    "#print(O)\n",
    "#O.reverse()\n",
    "#print(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(children_father)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "D_S_Set = {}\n",
    "C_S_Set = {} #colilation support dictionary\n",
    "T_S_Set = {}\n",
    "N_Set = {}\n",
    "C_N_Set = {}\n",
    "P_Childs= []\n",
    "#print(children_father['e5bi1ce'])\n",
    "\n",
    "#i=1\n",
    "def Reverse_P_Childs(P_Childs): \n",
    "    return [ele for ele in reversed(P_Childs)] \n",
    "      \n",
    "# Driver Code \n",
    "#lst = [10, 11, 12, 13, 14, 15]\n",
    "\n",
    "for node in reverse_O:\n",
    "    D_S_Set[node] = []\n",
    "    T_S_Set[node] = []\n",
    "    C_N_Set[node] = []\n",
    "    C_S_Set[node]= []\n",
    "    N_Set[node] = []\n",
    "    if node in father_children:\n",
    "        for child in father_children[node]:\n",
    "            f= father_children[node]\n",
    "            #if id_tag[child] == \"claim-support\":\n",
    "                #D_S_Set[node].append(child)\n",
    "              \n",
    "            if  id_tag[child] == \"claim-support\":\n",
    "                continue\n",
    "                P_Childs.append(child)\n",
    "                reverse_P_Childs=Reverse_P_Childs(P_Childs)\n",
    "                node_copy= node # copy of node, to get node childs\n",
    "                children_values=father_children.get(node) #get child values of current node from father_children dict.\n",
    "                for x in reverse_P_Childs: #iterate through reserve list of supporting nodes\n",
    "                    children_values=father_children.get(node_copy) #iteratively get children nodes \n",
    "                    if not children_values:\n",
    "                        continue\n",
    "                    else:\n",
    "                        for i in children_values:#compare nodes from the supporitng nodes and child node\n",
    "                            if x == i  and id_tag[i] == \"claim-support\":\n",
    "                                C_S_Set[node].append(x)\n",
    "                                node_copy=x\n",
    "            #elif id_tag[child] == \"claim-support\":\n",
    "                #C_S_Set[node].append(child)\n",
    "                                \n",
    "            elif id_tag[child] == \"claim-attack\":\n",
    "                N_Set[node].append(child)\n",
    "      \n",
    "    \n",
    "#print(C_N_Set)\n",
    "#print(\"the N_Set is\",N_Set)\n",
    "#print(D_S_Set)\n",
    "#print(\"the C_S_Set is\",C_S_Set)\n",
    "#print(\"the T_S_Set\",T_S_Set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResSet = []\n",
    "ignore_list = []\n",
    "for N in reverse_O: \n",
    "    #print(\"the N is\", N)\n",
    "    if id_tag[N] == \"issue\" or id_tag[N] == \"feature\" or id_tag[N] == 0:\n",
    "        #print(\"inside if first\")\n",
    "        result_Set = N_Set[N]\n",
    "        #print(\"the result set is\",result_Set)\n",
    "        for node in ignore_list:\n",
    "            if node in result_Set:\n",
    "                result_Set.remove(node)\n",
    "        if len(result_Set) != 0:\n",
    "            remained_N_Set = result_Set\n",
    "            temp = len(remained_N_Set)\n",
    "            for node in remained_N_Set:\n",
    "                if node_height[node] - node_height[N] > 2:\n",
    "                    temp -= 1\n",
    "            if len(remained_N_Set) != 0:\n",
    "                ignore_list.append(N) \n",
    "                for node in C_S_Set[N]:\n",
    "                    ignore_list.append(node)\n",
    "        result_Set = C_S_Set[N] + T_S_Set[N] + N_Set[N] + [N]\n",
    "        \n",
    "        for node in ignore_list:\n",
    "            if node in result_Set:\n",
    "                result_Set.remove(node)\n",
    "        ResSet.append(result_Set)\n",
    "    else:\n",
    "        if id_tag[N][:5] == \"claim\":\n",
    "            \n",
    "            \n",
    "            f = children_father[N]\n",
    "            #print(\"the negative set of N is\", N_Set[f])\n",
    "            if len(N_Set[N]) != 0:\n",
    "                #print(\"inside if\")\n",
    "                \n",
    "                for node in ignore_list:\n",
    "                    #print(\"N_Set[N] list is\",N_Set[N])\n",
    "                    if node in N_Set[N]:\n",
    "                        \n",
    "                        N_Set[N].remove(node)\n",
    "                if len(N_Set[N]) != 0:\n",
    "                    remained_N_Set = N_Set[N]\n",
    "                    \n",
    "                    temp = len(remained_N_Set)\n",
    "                    for node in remained_N_Set:                     \n",
    "                                               \n",
    "                        if node_height[node] - node_height[N] > 2:\n",
    "                            temp -= 1\n",
    "                    if temp != 0:\n",
    "                        ignore_list.append(N)\n",
    "                        #print(\"the C_S_Set[N]\", C_S_Set[N])\n",
    "                        for node in C_S_Set[N]:\n",
    "                            ignore_list.append(node)\n",
    "            if id_tag[N][-7:] == \"support\":\n",
    "                continue\n",
    "                #print(\"inside if of support/attack for \", N, \"and\", f)\n",
    "                #C_S_Set[f] = list(set(C_S_Set[f] + C_S_Set[N]))\n",
    "                #print(\"the numver C_S_Set[f] supporting is\",C_S_Set[f])\n",
    "                T_S_Set[f] = list(set(T_S_Set[f] + T_S_Set[N]))\n",
    "                #print(\"the numverT_S_Set[f] supporting is\",T_S_Set[f])\n",
    "                N_Set[f] = list(set(N_Set[f] + N_Set[N]))\n",
    "               \n",
    "            elif id_tag[N][-6:] == \"attack\":\n",
    "                N_Set[f] = list(set(N_Set[f] + C_S_Set[N]))\n",
    "                #print(\"the numver 1 negaive is\",N_Set[f], C_S_Set[N])\n",
    "                N_Set[f] = list(set(N_Set[f] + T_S_Set[N]))\n",
    "                #print(\"the numver 2 negaive is\",N_Set[f])\n",
    "                T_S_Set[f] = list(set(T_S_Set[f] + N_Set[N]))\n",
    "                #print(\"the numver 1 supproting is\",T_S_Set[f])\n",
    "#print(ResSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'9clgh7': ['e5bwzrb', 'e5d5dph', 'e5colkv', 'e5bhqef'],\n",
       " 'e5bh1by': ['e5bhwu7'],\n",
       " 'e5bi1ce': ['e5bu0gl'],\n",
       " 'e5bixvt': ['e5bqj1h'],\n",
       " 'e5bo0t5': ['e5c9tbc'],\n",
       " 'e5bo4qs': ['e5bqf79'],\n",
       " 'e5bpnup': ['e5bu127'],\n",
       " 'e5bqf79': ['e5bv2j3'],\n",
       " 'e5bs0cs': ['e5c0qe6'],\n",
       " 'e5bs6c7': ['e5c08ws'],\n",
       " 'e5c001z': ['e5chi75'],\n",
       " 'e5c08ws': ['e5c3xex'],\n",
       " 'e5c0qe6': ['e5c574t'],\n",
       " 'e5c574t': ['e5d7mqx'],\n",
       " 'e5crv0y': ['e5cv318']}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_sets2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['e5dbn4z'], ['e5dizid'], ['e5ciyid'], ['e5bo0t5', 'e5c9tbc'], ['e5diuxz'], ['e5d9jcp'], ['e5chm3o'], ['e5bqzlk'], ['e5bk8f0'], ['e5c2txt'], ['e5d9oa2'], ['e5bixvt', 'e5bqj1h'], ['e5choob'], ['e5d41nl'], ['e5bs0cs', 'e5c0qe6', 'e5c574t', 'e5d7mqx', 'e5bo4qs', 'e5bqf79', 'e5bv2j3', 'e5c03j5'], ['e5crv0y', 'e5cv318'], ['e5bhr9p'], ['e5cgv1o', 'e5d85sh', 'e5bs6c7', 'e5c08ws', 'e5c3xex'], ['e5c1e4u'], ['e5bumvv'], ['e5d237r'], ['e5d3fyg'], ['e5c9mr9'], ['e5bmpya'], ['e5btxu4'], ['e5bwn4g'], ['e5bqkhk'], ['e5bzx5i', 'e5bpnup', 'e5bu127', 'e5bh1by', 'e5bhwu7', 'e5cwhu1', 'e5d0jfz', 'e5cyx3h', 'e5djgug', 'e5d6vd7', 'e5dbmuv', 'e5bs4w3']]\n"
     ]
    }
   ],
   "source": [
    "def addItem(res, item_id):\n",
    "    if item_id not in res:\n",
    "        res.append(item_id)\n",
    "    if item_id in same_sets2:\n",
    "        for sub_id in same_sets2[item_id]:\n",
    "            res = addItem(res, sub_id)\n",
    "    return res\n",
    "\n",
    "new_resset = []\n",
    "for res in ResSet:\n",
    "    temp_set = []\n",
    "    for id in res:\n",
    "        temp_set = addItem(temp_set, id)\n",
    "    new_resset.append(temp_set)\n",
    "    \n",
    "print(new_resset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
