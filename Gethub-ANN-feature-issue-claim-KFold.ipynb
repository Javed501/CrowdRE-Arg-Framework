{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import keras\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "#nltk.download('popular')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from numpy import mean, std\n",
    "from sklearn.metrics import classification_report\n",
    "import plotly.graph_objs as go\n",
    "import plotly.plotly as py\n",
    "#import cufflinks\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import plotly.figure_factory as ff\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "from plotly.offline import iplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3051\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_Text</th>\n",
       "      <th>comment_author</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_upvotes</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>Rationale_Type</th>\n",
       "      <th>Claim_Type</th>\n",
       "      <th>Have_Rationale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>People still do racial checkbox ads?</td>\n",
       "      <td>PubicLouseInDaHouse</td>\n",
       "      <td>dfadazi</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>60v5za</td>\n",
       "      <td>claim</td>\n",
       "      <td>neu</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What?</td>\n",
       "      <td>Isvara</td>\n",
       "      <td>dfade5q</td>\n",
       "      <td>1.0</td>\n",
       "      <td>dfadazi</td>\n",
       "      <td>claim</td>\n",
       "      <td>neu</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90s power rangers.</td>\n",
       "      <td>PubicLouseInDaHouse</td>\n",
       "      <td>dfadlgi</td>\n",
       "      <td>1.0</td>\n",
       "      <td>dfade5q</td>\n",
       "      <td>claim</td>\n",
       "      <td>neu</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are there really uses for that?</td>\n",
       "      <td>piponwa</td>\n",
       "      <td>dfa8rlu</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>60v5za</td>\n",
       "      <td>issue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://support.google.com/plus/answer/2998354...</td>\n",
       "      <td>sparr</td>\n",
       "      <td>dfa365t</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>60v5za</td>\n",
       "      <td>claim</td>\n",
       "      <td>supporting</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_Text       comment_author  \\\n",
       "1               People still do racial checkbox ads?  PubicLouseInDaHouse   \n",
       "2                                              What?               Isvara   \n",
       "3                                 90s power rangers.  PubicLouseInDaHouse   \n",
       "4                    Are there really uses for that?              piponwa   \n",
       "5  https://support.google.com/plus/answer/2998354...                sparr   \n",
       "\n",
       "  comment_id  comment_upvotes parent_id Rationale_Type  Claim_Type  \\\n",
       "1    dfadazi             -1.0    60v5za          claim        neu    \n",
       "2    dfade5q              1.0   dfadazi          claim        neu    \n",
       "3    dfadlgi              1.0   dfade5q          claim        neu    \n",
       "4    dfa8rlu             -1.0    60v5za          issue         NaN   \n",
       "5    dfa365t             -1.0    60v5za          claim  supporting   \n",
       "\n",
       "  Have_Rationale  \n",
       "1             no  \n",
       "2             no  \n",
       "3             no  \n",
       "4            yes  \n",
       "5            yes  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('google-maps-single-dataset-for-conference.csv',encoding='ISO-8859-1')\n",
    "data= data[pd.notnull(data['Rationale_Type'])]\n",
    "data = data.replace(\"issue \", \"issue\")\n",
    "print(len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_plot(index):\n",
    "    example = data[data.index == index][['comment_Text', 'Rationale_Type']].values[0]\n",
    "    if len(example) > 0:\n",
    "        print(example[0])\n",
    "        print('Rationale_Type:', example[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> gender equality in any and all professions.\r\n",
      "\r\n",
      "Hahah uh... what?\r\n",
      "\r\n",
      "EDIT: Also I wasn't the one to downvote you above. No clue what that's about.\n",
      "Rationale_Type: claim\n"
     ]
    }
   ],
   "source": [
    "print_plot(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\<\\>\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z =#+_]')\n",
    "#STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_url(text):\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = strip_html(text) \n",
    "    text = remove_url(text)\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    #text = text.replace('x', '')\n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    #text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "data['comment_Text'] = data['comment_Text'].apply(clean_text)\n",
    "#df['Consumer complaint narrative'] = df['Consumer complaint narrative'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  gender equality in any and all professionshahah uh whatedit also i wasnt the one to downvote you above no clue what thats about\n",
      "Rationale_Type: claim\n"
     ]
    }
   ],
   "source": [
    "print_plot(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claim      2076\n",
      "feature     667\n",
      "issue       308\n",
      "Name: Rationale_Type, dtype: int64\n",
      "['claim' 'issue' 'feature']\n"
     ]
    }
   ],
   "source": [
    "print(data.Rationale_Type.value_counts())\n",
    "X= data.comment_Text.values.astype('U')\n",
    "y=data.Rationale_Type.values.astype('U')\n",
    "rationale_type=data.Rationale_Type\n",
    "rationale_type_name= data.Rationale_Type.unique()\n",
    "print(rationale_type_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size= 10000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "batch_size= 12\n",
    "epochs= 10\n",
    "test_accuracy =[]\n",
    "report_accuracy= []\n",
    "\n",
    "def evaluate_ANN_Model(trainX,trainy, testX, testy):\n",
    "    tokenizer= Tokenizer(num_words=vocab_size)\n",
    "    tokenizer.fit_on_texts(trainX)\n",
    "    X_train_matrix_tfidf= tokenizer.texts_to_matrix(trainX,mode='tfidf')\n",
    "    X_test_matrix_tfidf= tokenizer.texts_to_matrix(testX, mode='tfidf')\n",
    "\n",
    "    encoder= LabelBinarizer()\n",
    "    encoder.fit(trainy)\n",
    "    Y_train_matrix_tfidf= encoder.transform(trainy)\n",
    "    Y_test_matrix_tfidf= encoder.transform(testy)\n",
    "    smote = SMOTE()\n",
    "    X_sm_tfidf, y_sm_tfidf = smote.fit_sample(X_train_matrix_tfidf, Y_train_matrix_tfidf)\n",
    "    \n",
    "    #print(vocab_size)\n",
    "    model = Sequential()\n",
    "    #input layer\n",
    "    model.add(Dense(16, input_shape=(vocab_size, )))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.81))\n",
    "    #hidden layer\n",
    "    #odel.add(Dense(32))\n",
    "    #odel.add(Activation('relu'))\n",
    "    #odel.add(Dropout(0.3))\n",
    "    #output layer\n",
    "    model.add(Dense(3))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
    "    #model. summary()\n",
    "    #print(model.metrics_names)\n",
    "    #fit model \n",
    "    model.fit(X_sm_tfidf,y_sm_tfidf, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.2)\n",
    "    #evaluate model\n",
    "    score= model.evaluate(X_test_matrix_tfidf, Y_test_matrix_tfidf, batch_size=batch_size, verbose=1)\n",
    "    test_accuracy= score[1]\n",
    "    #predict model accuracy on testing data\n",
    "    pred = model.predict(X_test_matrix_tfidf, batch_size=12, verbose=1)\n",
    "    predicted = np.argmax(pred, axis=1)\n",
    "    report_accuracy = precision_recall_fscore_support(np.argmax(Y_test_matrix_tfidf, axis=1), predicted)\n",
    "    return model, test_accuracy, report_accuracy\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3283 samples, validate on 821 samples\n",
      "Epoch 1/10\n",
      "3283/3283 [==============================] - 9s 3ms/step - loss: 1.0487 - acc: 0.4496 - val_loss: 1.2456 - val_acc: 0.1705\n",
      "Epoch 2/10\n",
      "3283/3283 [==============================] - 2s 611us/step - loss: 0.8916 - acc: 0.5815 - val_loss: 1.0030 - val_acc: 0.5493\n",
      "Epoch 3/10\n",
      "3283/3283 [==============================] - 2s 643us/step - loss: 0.7935 - acc: 0.6339 - val_loss: 0.7751 - val_acc: 0.7247\n",
      "Epoch 4/10\n",
      "3283/3283 [==============================] - 2s 527us/step - loss: 0.7016 - acc: 0.6680 - val_loss: 0.6439 - val_acc: 0.7747\n",
      "Epoch 5/10\n",
      "3283/3283 [==============================] - 2s 519us/step - loss: 0.6779 - acc: 0.6762 - val_loss: 0.5477 - val_acc: 0.8088\n",
      "Epoch 6/10\n",
      "3283/3283 [==============================] - 2s 516us/step - loss: 0.6440 - acc: 0.7048 - val_loss: 0.4912 - val_acc: 0.8343\n",
      "Epoch 7/10\n",
      "3283/3283 [==============================] - 2s 567us/step - loss: 0.6104 - acc: 0.7091 - val_loss: 0.4561 - val_acc: 0.8307\n",
      "Epoch 8/10\n",
      "3283/3283 [==============================] - 2s 579us/step - loss: 0.5951 - acc: 0.7058 - val_loss: 0.4495 - val_acc: 0.8258\n",
      "Epoch 9/10\n",
      "3283/3283 [==============================] - 2s 509us/step - loss: 0.5583 - acc: 0.7243 - val_loss: 0.3464 - val_acc: 0.8952\n",
      "Epoch 10/10\n",
      "3283/3283 [==============================] - 2s 719us/step - loss: 0.5551 - acc: 0.7256 - val_loss: 0.3422 - val_acc: 0.8904\n",
      "1017/1017 [==============================] - 0s 261us/step\n",
      "1017/1017 [==============================] - 0s 473us/step\n",
      "the training accuracy is 0.6519174025473693\n",
      "the testing accuracy is (array([0.73200993, 0.33879781, 0.39285714]), array([0.83333333, 0.32291667, 0.09401709]), array([0.77939234, 0.33066667, 0.15172414]), array([708, 192, 117], dtype=int64))\n",
      "Train on 3494 samples, validate on 874 samples\n",
      "Epoch 1/10\n",
      "3494/3494 [==============================] - 4s 1ms/step - loss: 1.0003 - acc: 0.5094 - val_loss: 1.2935 - val_acc: 0.0366\n",
      "Epoch 2/10\n",
      "3494/3494 [==============================] - 2s 542us/step - loss: 0.8663 - acc: 0.6165 - val_loss: 1.1578 - val_acc: 0.2151\n",
      "Epoch 3/10\n",
      "3494/3494 [==============================] - 2s 493us/step - loss: 0.7663 - acc: 0.6657 - val_loss: 1.0424 - val_acc: 0.4863\n",
      "Epoch 4/10\n",
      "3494/3494 [==============================] - 2s 493us/step - loss: 0.6884 - acc: 0.7095 - val_loss: 0.9062 - val_acc: 0.6156\n",
      "Epoch 5/10\n",
      "3494/3494 [==============================] - 2s 545us/step - loss: 0.6384 - acc: 0.7264 - val_loss: 0.8459 - val_acc: 0.6865\n",
      "Epoch 6/10\n",
      "3494/3494 [==============================] - 2s 522us/step - loss: 0.6031 - acc: 0.7390 - val_loss: 0.7932 - val_acc: 0.7243\n",
      "Epoch 7/10\n",
      "3494/3494 [==============================] - 2s 602us/step - loss: 0.5719 - acc: 0.7642 - val_loss: 0.7148 - val_acc: 0.7803\n",
      "Epoch 8/10\n",
      "3494/3494 [==============================] - 2s 567us/step - loss: 0.5704 - acc: 0.7530 - val_loss: 0.6786 - val_acc: 0.8101\n",
      "Epoch 9/10\n",
      "3494/3494 [==============================] - 2s 617us/step - loss: 0.5335 - acc: 0.7736 - val_loss: 0.6373 - val_acc: 0.8215\n",
      "Epoch 10/10\n",
      "3494/3494 [==============================] - 2s 581us/step - loss: 0.5110 - acc: 0.7828 - val_loss: 0.5643 - val_acc: 0.8730\n",
      "1017/1017 [==============================] - 0s 192us/step\n",
      "1017/1017 [==============================] - 0s 412us/step\n",
      "the training accuracy is 0.6529006871853607\n",
      "the testing accuracy is (array([0.69220608, 0.57476636, 0.36956522]), array([0.84516129, 0.41836735, 0.16504854]), array([0.7610748 , 0.48425197, 0.22818792]), array([620, 294, 103], dtype=int64))\n",
      "Train on 3187 samples, validate on 797 samples\n",
      "Epoch 1/10\n",
      "3187/3187 [==============================] - 7s 2ms/step - loss: 1.0381 - acc: 0.4876 - val_loss: 1.2635 - val_acc: 0.0238\n",
      "Epoch 2/10\n",
      "3187/3187 [==============================] - 2s 605us/step - loss: 0.9044 - acc: 0.5799 - val_loss: 1.3159 - val_acc: 0.0402\n",
      "Epoch 3/10\n",
      "3187/3187 [==============================] - 2s 572us/step - loss: 0.8221 - acc: 0.6122 - val_loss: 1.2393 - val_acc: 0.0715\n",
      "Epoch 4/10\n",
      "3187/3187 [==============================] - 2s 598us/step - loss: 0.7439 - acc: 0.6589 - val_loss: 1.1861 - val_acc: 0.1242\n",
      "Epoch 5/10\n",
      "3187/3187 [==============================] - 2s 621us/step - loss: 0.7344 - acc: 0.6643 - val_loss: 1.1083 - val_acc: 0.1493\n",
      "Epoch 6/10\n",
      "3187/3187 [==============================] - 2s 550us/step - loss: 0.6813 - acc: 0.6806 - val_loss: 1.0623 - val_acc: 0.2070\n",
      "Epoch 7/10\n",
      "3187/3187 [==============================] - 2s 590us/step - loss: 0.6505 - acc: 0.7025 - val_loss: 0.9459 - val_acc: 0.3639\n",
      "Epoch 8/10\n",
      "3187/3187 [==============================] - 2s 570us/step - loss: 0.6213 - acc: 0.7170 - val_loss: 0.9037 - val_acc: 0.4366\n",
      "Epoch 9/10\n",
      "3187/3187 [==============================] - 2s 544us/step - loss: 0.6081 - acc: 0.7185 - val_loss: 0.8317 - val_acc: 0.5571\n",
      "Epoch 10/10\n",
      "3187/3187 [==============================] - 2s 513us/step - loss: 0.5896 - acc: 0.7308 - val_loss: 0.8394 - val_acc: 0.5006\n",
      "1017/1017 [==============================] - 0s 186us/step\n",
      "1017/1017 [==============================] - 0s 482us/step\n",
      "the training accuracy is 0.7325467048844757\n",
      "the testing accuracy is (array([0.80665025, 0.44845361, 0.27272727]), array([0.87566845, 0.48066298, 0.03409091]), array([0.83974359, 0.464     , 0.06060606]), array([748, 181,  88], dtype=int64))\n",
      "the average Training Accuracy of ANN-TFIDF is 0.6791215982057351\n",
      "the averag Validation accuracy of ANN-TFIDF is [[7.43622083e-01 4.54005926e-01 3.45049878e-01]\n",
      " [8.51387691e-01 4.07315666e-01 9.77188489e-02]\n",
      " [7.93403576e-01 4.26306212e-01 1.46839373e-01]\n",
      " [6.92000000e+02 2.22333333e+02 1.02666667e+02]]\n"
     ]
    }
   ],
   "source": [
    "#Kfold ANN with TFIDF\n",
    "\n",
    "kf= KFold(n_splits=3)\n",
    "#curr_fold= 0\n",
    "alg_accurcy_ANN_tfidf =[]\n",
    "alg_testing_accurcy_ANN_tfidf =[]\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test= X[train_idx], X[test_idx]\n",
    "    y_train, y_test= y[train_idx], y[test_idx]\n",
    "        \n",
    "    model, test_accuracy,report_accuracy = evaluate_ANN_Model(X_train,y_train, X_test, y_test )\n",
    "    print (\"the training accuracy is\",test_accuracy )\n",
    "    alg_accurcy_ANN_tfidf.append(test_accuracy)\n",
    "    print (\"the testing accuracy is\",report_accuracy )\n",
    "    alg_testing_accurcy_ANN_tfidf.append(report_accuracy)\n",
    "    \n",
    "\n",
    "average_accuracy_ANN_training= np.mean(alg_accurcy_ANN_tfidf, axis=0)\n",
    "print(\"the average Training Accuracy of ANN-TFIDF is\",average_accuracy_ANN_training) \n",
    "\n",
    "average_accuracy_ANN_testing= np.mean(alg_testing_accurcy_ANN_tfidf, axis=0)\n",
    "print(\"the averag Validation accuracy of ANN-TFIDF is\",average_accuracy_ANN_testing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size= 10000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "batch_size= 12\n",
    "epochs= 10\n",
    "test_accuracy =[]\n",
    "report_accuracy= []\n",
    "\n",
    "def evaluate_ANN_Model(trainX,trainy, testX, testy):\n",
    "    \n",
    "    \n",
    "    tokenizer= Tokenizer(num_words=vocab_size)\n",
    "    tokenizer.fit_on_texts(trainX)\n",
    "    X_train_matrix_binary= tokenizer.texts_to_matrix(trainX,mode='binary')\n",
    "    X_test_matrix_binary= tokenizer.texts_to_matrix(testX, mode='binary')\n",
    "\n",
    "    encoder= LabelBinarizer()\n",
    "    encoder.fit(trainy)\n",
    "    Y_train_matrix_binary= encoder.transform(trainy)\n",
    "    Y_test_matrix_binary= encoder.transform(testy)\n",
    "    \n",
    "    \n",
    "    smote = SMOTE()\n",
    "    X_sm_binary, y_sm_binary = smote.fit_sample(X_train_matrix_binary, Y_train_matrix_binary)\n",
    "    \n",
    "    MAX_SEQUENCE_LENGTH = 1000\n",
    "    #print(vocab_size)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_shape=(vocab_size, )))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.85))\n",
    "    #hidden layer\n",
    "    #model.add(Dense(16))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(3))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
    "    #model. summary()\n",
    "    #print(model.metrics_names)\n",
    "\n",
    "    model.fit(X_sm_binary,y_sm_binary, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.2)\n",
    "    score= model.evaluate(X_test_matrix_binary, Y_test_matrix_binary, batch_size=batch_size, verbose=1)\n",
    "    test_accuracy= score[1]\n",
    "    #predict model accuracy on testing data\n",
    "    pred = model.predict(X_test_matrix_binary, batch_size=12, verbose=1)\n",
    "    predicted = np.argmax(pred, axis=1)\n",
    "    report_accuracy = precision_recall_fscore_support(np.argmax(Y_test_matrix_binary, axis=1), predicted)\n",
    "    return model, test_accuracy, report_accuracy  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2517 samples, validate on 630 samples\n",
      "Epoch 1/10\n",
      "2517/2517 [==============================] - 2s 984us/step - loss: 1.0422 - acc: 0.4474 - val_loss: 1.3279 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "2517/2517 [==============================] - 1s 514us/step - loss: 0.9494 - acc: 0.5491 - val_loss: 1.2867 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "2517/2517 [==============================] - 1s 510us/step - loss: 0.8762 - acc: 0.5952 - val_loss: 1.3321 - val_acc: 0.0095\n",
      "Epoch 4/10\n",
      "2517/2517 [==============================] - 1s 524us/step - loss: 0.8184 - acc: 0.6408 - val_loss: 1.2778 - val_acc: 0.0127\n",
      "Epoch 5/10\n",
      "2517/2517 [==============================] - 1s 500us/step - loss: 0.7642 - acc: 0.6611 - val_loss: 1.1915 - val_acc: 0.0381\n",
      "Epoch 6/10\n",
      "2517/2517 [==============================] - 1s 503us/step - loss: 0.7248 - acc: 0.6834 - val_loss: 1.1503 - val_acc: 0.0984\n",
      "Epoch 7/10\n",
      "2517/2517 [==============================] - 1s 508us/step - loss: 0.7040 - acc: 0.6921 - val_loss: 1.0607 - val_acc: 0.2746\n",
      "Epoch 8/10\n",
      "2517/2517 [==============================] - 1s 509us/step - loss: 0.6609 - acc: 0.6945 - val_loss: 1.0308 - val_acc: 0.2937\n",
      "Epoch 9/10\n",
      "2517/2517 [==============================] - 1s 506us/step - loss: 0.6411 - acc: 0.7060 - val_loss: 0.9422 - val_acc: 0.4508\n",
      "Epoch 10/10\n",
      "2517/2517 [==============================] - 1s 516us/step - loss: 0.6264 - acc: 0.7203 - val_loss: 0.9083 - val_acc: 0.4730\n",
      "1526/1526 [==============================] - 0s 215us/step\n",
      "1526/1526 [==============================] - 1s 346us/step\n",
      "the training accuracy is 0.6703800786564891\n",
      "the testing accuracy is (array([0.72181671, 0.45964912, 0.25      ]), array([0.86660175, 0.38081395, 0.01290323]), array([0.78761062, 0.41653418, 0.02453988]), array([1027,  344,  155], dtype=int64))\n",
      "Train on 2464 samples, validate on 617 samples\n",
      "Epoch 1/10\n",
      "2464/2464 [==============================] - 3s 1ms/step - loss: 1.0596 - acc: 0.4407 - val_loss: 1.3022 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "2464/2464 [==============================] - 1s 537us/step - loss: 0.9766 - acc: 0.5207 - val_loss: 1.3710 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "2464/2464 [==============================] - 1s 539us/step - loss: 0.8997 - acc: 0.5637 - val_loss: 1.3628 - val_acc: 0.0016\n",
      "Epoch 4/10\n",
      "2464/2464 [==============================] - 1s 533us/step - loss: 0.8528 - acc: 0.5877 - val_loss: 1.2880 - val_acc: 0.0130\n",
      "Epoch 5/10\n",
      "2464/2464 [==============================] - 1s 531us/step - loss: 0.7943 - acc: 0.6071 - val_loss: 1.1820 - val_acc: 0.0502\n",
      "Epoch 6/10\n",
      "2464/2464 [==============================] - 1s 530us/step - loss: 0.7567 - acc: 0.6465 - val_loss: 1.0945 - val_acc: 0.1280\n",
      "Epoch 7/10\n",
      "2464/2464 [==============================] - 1s 547us/step - loss: 0.7096 - acc: 0.6680 - val_loss: 1.0827 - val_acc: 0.1248\n",
      "Epoch 8/10\n",
      "2464/2464 [==============================] - 1s 576us/step - loss: 0.7024 - acc: 0.6778 - val_loss: 1.0098 - val_acc: 0.2318\n",
      "Epoch 9/10\n",
      "2464/2464 [==============================] - 1s 571us/step - loss: 0.6592 - acc: 0.6976 - val_loss: 0.9442 - val_acc: 0.3128\n",
      "Epoch 10/10\n",
      "2464/2464 [==============================] - 1s 549us/step - loss: 0.6480 - acc: 0.6981 - val_loss: 0.9041 - val_acc: 0.3987\n",
      "1525/1525 [==============================] - 0s 288us/step\n",
      "1525/1525 [==============================] - 1s 359us/step\n",
      "the training accuracy is 0.7245901629963859\n",
      "the testing accuracy is (array([0.75293198, 0.5787234 , 0.54545455]), array([0.91801716, 0.42105263, 0.03921569]), array([0.82731959, 0.4874552 , 0.07317073]), array([1049,  323,  153], dtype=int64))\n",
      "the average Training Accuracy of ANN-Binary is 0.6791215982057351\n",
      "the averag Validation accuracy of ANN-Binary is [[7.43622083e-01 4.54005926e-01 3.45049878e-01]\n",
      " [8.51387691e-01 4.07315666e-01 9.77188489e-02]\n",
      " [7.93403576e-01 4.26306212e-01 1.46839373e-01]\n",
      " [6.92000000e+02 2.22333333e+02 1.02666667e+02]]\n"
     ]
    }
   ],
   "source": [
    "#Kfold ANN with mode=binary\n",
    "\n",
    "kf= KFold(n_splits=2)\n",
    "#curr_fold= 0\n",
    "alg_accurcy_ANN_binary =[]\n",
    "alg_testing_accurcy_ANN_binary =[]\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test= X[train_idx], X[test_idx]\n",
    "    y_train, y_test= y[train_idx], y[test_idx]\n",
    "        \n",
    "    model, test_accuracy,report_accuracy = evaluate_ANN_Model(X_train,y_train, X_test, y_test )\n",
    "    print (\"the training accuracy is\",test_accuracy )\n",
    "    alg_accurcy_ANN_binary.append(test_accuracy)\n",
    "    print (\"the testing accuracy is\",report_accuracy )\n",
    "    alg_testing_accurcy_ANN_binary.append(report_accuracy)\n",
    "    \n",
    "\n",
    "average_accuracy_ANN_binary= np.mean(alg_accurcy_ANN_tfidf, axis=0)\n",
    "print(\"the average Training Accuracy of ANN-Binary is\",average_accuracy_ANN_binary) \n",
    "\n",
    "average_accuracy_ANN_binary= np.mean(alg_testing_accurcy_ANN_tfidf, axis=0)\n",
    "print(\"the averag Validation accuracy of ANN-Binary is\",average_accuracy_ANN_binary)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size= 10000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "batch_size= 12\n",
    "epochs= 10\n",
    "test_accuracy =[]\n",
    "report_accuracy= []\n",
    "\n",
    "def evaluate_ANN_Model(trainX,trainy, testX, testy):\n",
    "    \n",
    "    tokenizer= Tokenizer(num_words=vocab_size)\n",
    "    tokenizer.fit_on_texts(trainX)\n",
    "    X_train_matrix_count= tokenizer.texts_to_matrix(trainX,mode='count')\n",
    "    X_test_matrix_count= tokenizer.texts_to_matrix(testX, mode='count')\n",
    "\n",
    "    encoder= LabelBinarizer()\n",
    "    encoder.fit(trainy)\n",
    "    Y_train_matrix_count= encoder.transform(trainy)\n",
    "    Y_test_matrix_count= encoder.transform(testy)     \n",
    "    \n",
    "    smote = SMOTE('minority')\n",
    "    X_sm_count, y_sm_count = smote.fit_sample(X_train_matrix_count, Y_train_matrix_count)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_shape=(vocab_size, )))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.7))\n",
    "    model.add(Dense(3))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
    "    model. summary()\n",
    "    #print(model.metrics_names)\n",
    "    from sklearn.utils import class_weight\n",
    "    class_weight= class_weight.compute_class_weight('balanced', np.unique(rationale_type),rationale_type)\n",
    "    #print(class_weight)\n",
    "    model.fit(X_sm_count,y_sm_count, batch_size=batch_size, class_weight=class_weight, epochs=epochs, verbose=1, validation_split=0.3)\n",
    "    score=model.evaluate(X_test_matrix_count, Y_test_matrix_count, batch_size=batch_size, verbose=1)\n",
    "    test_accuracy= score[1]\n",
    "    #predict model accuracy on testing data\n",
    "    pred = model.predict(X_test_matrix_count, batch_size=12, verbose=1)\n",
    "    predicted = np.argmax(pred, axis=1)\n",
    "    report_accuracy = precision_recall_fscore_support(np.argmax(Y_test_matrix_count, axis=1), predicted)\n",
    "    return model, test_accuracy, report_accuracy  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_63 (Dense)             (None, 16)                160016    \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 3)                 51        \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 160,067\n",
      "Trainable params: 160,067\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1539 samples, validate on 660 samples\n",
      "Epoch 1/10\n",
      "1539/1539 [==============================] - 8s 5ms/step - loss: 1.0160 - acc: 0.5724 - val_loss: 1.2911 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1539/1539 [==============================] - 2s 996us/step - loss: 0.9035 - acc: 0.6127 - val_loss: 1.2930 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1539/1539 [==============================] - 1s 707us/step - loss: 0.8124 - acc: 0.6329 - val_loss: 1.2411 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1539/1539 [==============================] - 1s 637us/step - loss: 0.7459 - acc: 0.6725 - val_loss: 1.1162 - val_acc: 0.0697\n",
      "Epoch 5/10\n",
      "1539/1539 [==============================] - 1s 672us/step - loss: 0.6861 - acc: 0.6953 - val_loss: 0.9883 - val_acc: 0.3788\n",
      "Epoch 6/10\n",
      "1539/1539 [==============================] - 1s 652us/step - loss: 0.6225 - acc: 0.7505 - val_loss: 0.8127 - val_acc: 0.6909\n",
      "Epoch 7/10\n",
      "1539/1539 [==============================] - 1s 632us/step - loss: 0.5640 - acc: 0.7804 - val_loss: 0.7443 - val_acc: 0.7712\n",
      "Epoch 8/10\n",
      "1539/1539 [==============================] - 1s 606us/step - loss: 0.5096 - acc: 0.7986 - val_loss: 0.6639 - val_acc: 0.8515\n",
      "Epoch 9/10\n",
      "1539/1539 [==============================] - 1s 632us/step - loss: 0.4616 - acc: 0.8122 - val_loss: 0.4926 - val_acc: 0.9364\n",
      "Epoch 10/10\n",
      "1539/1539 [==============================] - 1s 619us/step - loss: 0.4456 - acc: 0.8181 - val_loss: 0.4800 - val_acc: 0.9182\n",
      "1377/1377 [==============================] - 0s 267us/step\n",
      "1377/1377 [==============================] - 2s 1ms/step\n",
      "the training accuracy is 0.6608569319471554\n",
      "the testing accuracy is (array([0.68683565, 0.4957265 , 0.32432432]), array([0.92004381, 0.1835443 , 0.08108108]), array([0.78651685, 0.26789838, 0.12972973]), array([913, 316, 148], dtype=int64))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_65 (Dense)             (None, 16)                160016    \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 3)                 51        \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 160,067\n",
      "Trainable params: 160,067\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1499 samples, validate on 643 samples\n",
      "Epoch 1/10\n",
      "1499/1499 [==============================] - 9s 6ms/step - loss: 1.0142 - acc: 0.5851 - val_loss: 1.2720 - val_acc: 0.0000e+00 0s - loss: 1.0085 - acc: 0.5\n",
      "Epoch 2/10\n",
      "1499/1499 [==============================] - 3s 2ms/step - loss: 0.9121 - acc: 0.6064 - val_loss: 1.2726 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1499/1499 [==============================] - 2s 1ms/step - loss: 0.8495 - acc: 0.6104 - val_loss: 1.2697 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1499/1499 [==============================] - 1s 991us/step - loss: 0.7942 - acc: 0.6231 - val_loss: 1.2026 - val_acc: 0.0156\n",
      "Epoch 5/10\n",
      "1499/1499 [==============================] - 1s 678us/step - loss: 0.7372 - acc: 0.6404 - val_loss: 1.1142 - val_acc: 0.1384\n",
      "Epoch 6/10\n",
      "1499/1499 [==============================] - 1s 683us/step - loss: 0.6924 - acc: 0.6604 - val_loss: 1.0307 - val_acc: 0.2255\n",
      "Epoch 7/10\n",
      "1499/1499 [==============================] - 1s 681us/step - loss: 0.6468 - acc: 0.6885 - val_loss: 0.9407 - val_acc: 0.4339\n",
      "Epoch 8/10\n",
      "1499/1499 [==============================] - 1s 671us/step - loss: 0.5914 - acc: 0.7278 - val_loss: 0.7700 - val_acc: 0.8134\n",
      "Epoch 9/10\n",
      "1499/1499 [==============================] - 1s 672us/step - loss: 0.5633 - acc: 0.7425 - val_loss: 0.7208 - val_acc: 0.8367\n",
      "Epoch 10/10\n",
      "1499/1499 [==============================] - 1s 701us/step - loss: 0.5298 - acc: 0.7658 - val_loss: 0.6338 - val_acc: 0.8865\n",
      "1376/1376 [==============================] - 0s 339us/step\n",
      "1376/1376 [==============================] - 2s 1ms/step\n",
      "the training accuracy is 0.694767442510225\n",
      "the testing accuracy is (array([0.71507937, 0.57142857, 0.21875   ]), array([0.94742376, 0.16161616, 0.0546875 ]), array([0.81501583, 0.2519685 , 0.0875    ]), array([951, 297, 128], dtype=int64))\n",
      "the average Training Accuracy of ANN-Binary is 0.6778121872286902\n",
      "the averag Validation accuracy of ANN-Binary is [[7.00957508e-01 5.33577534e-01 2.71537162e-01]\n",
      " [9.33733788e-01 1.72580233e-01 6.78842905e-02]\n",
      " [8.00766342e-01 2.59933444e-01 1.08614865e-01]\n",
      " [9.32000000e+02 3.06500000e+02 1.38000000e+02]]\n"
     ]
    }
   ],
   "source": [
    "#Kfold ANN with mode=count\n",
    "\n",
    "kf= KFold(n_splits=2)\n",
    "#curr_fold= 0\n",
    "alg_accurcy_ANN_count =[]\n",
    "alg_testing_accurcy_ANN_count =[]\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test= X[train_idx], X[test_idx]\n",
    "    y_train, y_test= y[train_idx], y[test_idx]\n",
    "        \n",
    "    model, test_accuracy,report_accuracy = evaluate_ANN_Model(X_train,y_train, X_test, y_test )\n",
    "    print (\"the training accuracy is\",test_accuracy )\n",
    "    alg_accurcy_ANN_count.append(test_accuracy)\n",
    "    print (\"the testing accuracy is\",report_accuracy )\n",
    "    alg_testing_accurcy_ANN_count.append(report_accuracy)\n",
    "    \n",
    "\n",
    "average_accuracy_ANN_count= np.mean(alg_accurcy_ANN_count, axis=0)\n",
    "print(\"the average Training Accuracy of ANN-Binary is\",average_accuracy_ANN_count) \n",
    "\n",
    "average_accuracy_ANN_count= np.mean(alg_testing_accurcy_ANN_count, axis=0)\n",
    "print(\"the averag Validation accuracy of ANN-Binary is\",average_accuracy_ANN_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
