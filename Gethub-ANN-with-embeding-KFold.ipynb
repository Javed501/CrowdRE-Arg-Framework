{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import keras\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "#nltk.download('popular')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, Flatten,Input \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from numpy import mean, std\n",
    "from sklearn.metrics import classification_report\n",
    "import plotly.graph_objs as go\n",
    "import plotly.plotly as py\n",
    "#import cufflinks\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import plotly.figure_factory as ff\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "from plotly.offline import iplot\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3051\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_Text</th>\n",
       "      <th>comment_author</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_upvotes</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>Rationale_Type</th>\n",
       "      <th>Claim_Type</th>\n",
       "      <th>Have_Rationale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>People still do racial checkbox ads?</td>\n",
       "      <td>PubicLouseInDaHouse</td>\n",
       "      <td>dfadazi</td>\n",
       "      <td>-1</td>\n",
       "      <td>60v5za</td>\n",
       "      <td>claim</td>\n",
       "      <td>neu</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What?</td>\n",
       "      <td>Isvara</td>\n",
       "      <td>dfade5q</td>\n",
       "      <td>1</td>\n",
       "      <td>dfadazi</td>\n",
       "      <td>claim</td>\n",
       "      <td>neu</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90s power rangers.</td>\n",
       "      <td>PubicLouseInDaHouse</td>\n",
       "      <td>dfadlgi</td>\n",
       "      <td>1</td>\n",
       "      <td>dfade5q</td>\n",
       "      <td>claim</td>\n",
       "      <td>neu</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are there really uses for that?</td>\n",
       "      <td>piponwa</td>\n",
       "      <td>dfa8rlu</td>\n",
       "      <td>-1</td>\n",
       "      <td>60v5za</td>\n",
       "      <td>issue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://support.google.com/plus/answer/2998354...</td>\n",
       "      <td>sparr</td>\n",
       "      <td>dfa365t</td>\n",
       "      <td>-1</td>\n",
       "      <td>60v5za</td>\n",
       "      <td>claim</td>\n",
       "      <td>supporting</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_Text       comment_author  \\\n",
       "1               People still do racial checkbox ads?  PubicLouseInDaHouse   \n",
       "2                                              What?               Isvara   \n",
       "3                                 90s power rangers.  PubicLouseInDaHouse   \n",
       "4                    Are there really uses for that?              piponwa   \n",
       "5  https://support.google.com/plus/answer/2998354...                sparr   \n",
       "\n",
       "  comment_id  comment_upvotes parent_id Rationale_Type  Claim_Type  \\\n",
       "1    dfadazi               -1    60v5za          claim        neu    \n",
       "2    dfade5q                1   dfadazi          claim        neu    \n",
       "3    dfadlgi                1   dfade5q          claim        neu    \n",
       "4    dfa8rlu               -1    60v5za          issue         NaN   \n",
       "5    dfa365t               -1    60v5za          claim  supporting   \n",
       "\n",
       "  Have_Rationale  \n",
       "1             no  \n",
       "2             no  \n",
       "3             no  \n",
       "4            yes  \n",
       "5            yes  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('google-maps-single-dataset-for-conference.csv',encoding='ISO-8859-1')\n",
    "data= data[pd.notnull(data['Rationale_Type'])]\n",
    "data = data.replace(\"issue \", \"issue\")\n",
    "print(len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_plot(index):\n",
    "    example = data[data.index == index][['comment_Text', 'Rationale_Type']].values[0]\n",
    "    if len(example) > 0:\n",
    "        print(example[0])\n",
    "        print('Rationale_Type:', example[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> gender equality in any and all professions.\r\n",
      "\r\n",
      "Hahah uh... what?\r\n",
      "\r\n",
      "EDIT: Also I wasn't the one to downvote you above. No clue what that's about.\n",
      "Rationale_Type: claim\n"
     ]
    }
   ],
   "source": [
    "print_plot(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\<\\>\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z =#+_]')\n",
    "#STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_url(text):\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = strip_html(text) \n",
    "    text = remove_url(text)\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    #text = text.replace('x', '')\n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    #text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "data['comment_Text'] = data['comment_Text'].apply(clean_text)\n",
    "#df['Consumer complaint narrative'] = df['Consumer complaint narrative'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  gender equality in any and all professionshahah uh whatedit also i wasnt the one to downvote you above no clue what thats about\n",
      "Rationale_Type: claim\n"
     ]
    }
   ],
   "source": [
    "print_plot(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2076\n",
      "1     667\n",
      "2     308\n",
      "Name: Rationale_Type, dtype: int64\n",
      "[0 2 1]\n"
     ]
    }
   ],
   "source": [
    "print(data.Rationale_Type.value_counts())\n",
    "#X= data.comment_Text.values.astype('U')\n",
    "X= data.comment_Text.values.astype('U')\n",
    "y=data.Rationale_Type.values.astype('U')\n",
    "rationale_type=data.Rationale_Type\n",
    "rationale_type_name= data.Rationale_Type.unique()\n",
    "print(rationale_type_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "macronum=sorted(set(data['Rationale_Type']))\n",
    "print(macronum)\n",
    "macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n",
    "\n",
    "def fun(i):\n",
    "    return macro_to_id[i]\n",
    "\n",
    "data['Rationale_Type']=data['Rationale_Type'].apply(fun)\n",
    "#print(data['Rationale_Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "rationale_labels = []\n",
    "for idx in data['Rationale_Type']:\n",
    "    rationale_labels.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Tokens 7030\n",
      "(3051, 1000) (3051, 3)\n"
     ]
    }
   ],
   "source": [
    "vocab_size= 10000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "tokenizer= Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences_ANN = tokenizer.texts_to_sequences(X)\n",
    "word_index = tokenizer.word_index\n",
    "print('Number of Unique Tokens',len(word_index))\n",
    "final_data_ANN = pad_sequences(sequences_ANN, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "#print(final_data_ANN)\n",
    "rationale_labels_ANN=to_categorical(np.asarray(rationale_labels))\n",
    "print(final_data_ANN.shape, rationale_labels_ANN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size= 10000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "batch_size= 12\n",
    "epochs= 10\n",
    "\n",
    "test_accuracy =[]\n",
    "report_accuracy= []\n",
    "\n",
    "def evaluate_ANN_Model(trainX,trainy, testX, testy):\n",
    "    \n",
    "    \n",
    "    embeddings_index = {}\n",
    "    f = open('glove.6B.100d.txt',encoding='utf8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))\n",
    "    \n",
    "    \n",
    "    EMBEDDING_DIM=100\n",
    "    MAX_SEQUENCE_LENGTH = 1000\n",
    "    #embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    embedding_matrix = zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "    \n",
    "    \n",
    "    smote = SMOTE('minority')\n",
    "    X_sm_embedding, y_sm_embedding = smote.fit_sample(trainX, trainy)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(embedding_layer) \n",
    "    model.add(Flatten())\n",
    "    #input layer\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.35))\n",
    "    \n",
    "    model.add(Dense(3))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer= 'SGD', metrics=['accuracy'])  \n",
    "    \n",
    "    #class_weight= class_weight.compute_class_weight('balanced', np.unique(rationale_type),rationale_type)\n",
    "    \n",
    "   \n",
    "    #model.compile(loss='categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
    "     \n",
    "    model.fit(X_sm_embedding,y_sm_embedding, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.2)\n",
    "    \n",
    "\n",
    "    #evaluate model\n",
    "    score= model.evaluate(testX, testy, batch_size=batch_size, verbose=1)\n",
    "    test_accuracy= score[1]\n",
    "    #predict model accuracy on testing data\n",
    "    pred = model.predict(testX, batch_size=12, verbose=1)\n",
    "    predicted = np.argmax(pred, axis=1)\n",
    "    report_accuracy = precision_recall_fscore_support(np.argmax(testy, axis=1), predicted)\n",
    "    return model, test_accuracy, report_accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors in Glove 6B 100d.\n",
      "Train on 2568 samples, validate on 643 samples\n",
      "Epoch 1/10\n",
      "2568/2568 [==============================] - 8s 3ms/step - loss: 1.0347 - acc: 0.5101 - val_loss: 1.2855 - val_acc: 0.0062\n",
      "Epoch 2/10\n",
      "2568/2568 [==============================] - 4s 2ms/step - loss: 0.9747 - acc: 0.5448 - val_loss: 1.0873 - val_acc: 0.1353\n",
      "Epoch 3/10\n",
      "2568/2568 [==============================] - 4s 2ms/step - loss: 0.9223 - acc: 0.5666 - val_loss: 1.1099 - val_acc: 0.1602\n",
      "Epoch 4/10\n",
      "2568/2568 [==============================] - 4s 2ms/step - loss: 0.8691 - acc: 0.5876 - val_loss: 1.5025 - val_acc: 0.0762\n",
      "Epoch 5/10\n",
      "2568/2568 [==============================] - 5s 2ms/step - loss: 0.8179 - acc: 0.6090 - val_loss: 0.8847 - val_acc: 0.5956- acc: 0.606 - ETA: 1s - loss: \n",
      "Epoch 6/10\n",
      "2568/2568 [==============================] - 4s 1ms/step - loss: 0.7798 - acc: 0.6355 - val_loss: 1.0858 - val_acc: 0.4277\n",
      "Epoch 7/10\n",
      "2568/2568 [==============================] - 4s 1ms/step - loss: 0.7493 - acc: 0.6562 - val_loss: 1.1972 - val_acc: 0.3453\n",
      "Epoch 8/10\n",
      "2568/2568 [==============================] - 4s 1ms/step - loss: 0.7051 - acc: 0.6783 - val_loss: 0.6996 - val_acc: 0.7325\n",
      "Epoch 9/10\n",
      "2568/2568 [==============================] - 4s 1ms/step - loss: 0.6700 - acc: 0.7025 - val_loss: 0.8375 - val_acc: 0.6532\n",
      "Epoch 10/10\n",
      "2568/2568 [==============================] - 4s 1ms/step - loss: 0.6219 - acc: 0.7227 - val_loss: 0.9084 - val_acc: 0.6034\n",
      "1017/1017 [==============================] - 0s 341us/step\n",
      "1017/1017 [==============================] - 1s 637us/step\n",
      "the training accuracy is 0.5889872163094602\n",
      "the testing accuracy is (array([0.72572178, 0.34090909, 0.14691943]), array([0.78107345, 0.078125  , 0.26495726]), array([0.75238095, 0.12711864, 0.18902439]), array([708, 192, 117], dtype=int64))\n",
      "Total 400000 word vectors in Glove 6B 100d.\n",
      "Train on 2628 samples, validate on 657 samples\n",
      "Epoch 1/10\n",
      "2628/2628 [==============================] - 10s 4ms/step - loss: 0.9886 - acc: 0.5434 - val_loss: 0.9708 - val_acc: 0.1948\n",
      "Epoch 2/10\n",
      "2628/2628 [==============================] - 4s 2ms/step - loss: 0.9248 - acc: 0.5757 - val_loss: 0.9561 - val_acc: 0.1613\n",
      "Epoch 3/10\n",
      "2628/2628 [==============================] - 4s 1ms/step - loss: 0.8717 - acc: 0.6005 - val_loss: 0.9405 - val_acc: 0.3714\n",
      "Epoch 4/10\n",
      "2628/2628 [==============================] - 4s 1ms/step - loss: 0.8264 - acc: 0.6313 - val_loss: 0.9608 - val_acc: 0.4399\n",
      "Epoch 5/10\n",
      "2628/2628 [==============================] - 4s 2ms/step - loss: 0.7699 - acc: 0.6579 - val_loss: 1.0178 - val_acc: 0.3760\n",
      "Epoch 6/10\n",
      "2628/2628 [==============================] - 4s 1ms/step - loss: 0.7295 - acc: 0.6754 - val_loss: 0.8875 - val_acc: 0.5906\n",
      "Epoch 7/10\n",
      "2628/2628 [==============================] - 4s 1ms/step - loss: 0.6685 - acc: 0.7100 - val_loss: 1.2785 - val_acc: 0.3592\n",
      "Epoch 8/10\n",
      "2628/2628 [==============================] - 4s 1ms/step - loss: 0.6377 - acc: 0.7180 - val_loss: 0.7482 - val_acc: 0.6834\n",
      "Epoch 9/10\n",
      "2628/2628 [==============================] - 4s 1ms/step - loss: 0.6138 - acc: 0.7401 - val_loss: 0.6443 - val_acc: 0.7352cc: 0.7\n",
      "Epoch 10/10\n",
      "2628/2628 [==============================] - 4s 1ms/step - loss: 0.5536 - acc: 0.7789 - val_loss: 0.8125 - val_acc: 0.6225\n",
      "1017/1017 [==============================] - 0s 330us/step\n",
      "1017/1017 [==============================] - 1s 655us/step\n",
      "the training accuracy is 0.5172074720219525\n",
      "the testing accuracy is (array([0.64404432, 0.55555556, 0.15830116]), array([0.75      , 0.06802721, 0.39805825]), array([0.69299553, 0.12121212, 0.22651934]), array([620, 294, 103], dtype=int64))\n",
      "Total 400000 word vectors in Glove 6B 100d.\n",
      "Train on 2513 samples, validate on 629 samples\n",
      "Epoch 1/10\n",
      "2513/2513 [==============================] - 9s 3ms/step - loss: 1.0447 - acc: 0.5018 - val_loss: 1.2679 - val_acc: 0.0079\n",
      "Epoch 2/10\n",
      "2513/2513 [==============================] - 4s 2ms/step - loss: 0.9811 - acc: 0.5388 - val_loss: 0.9786 - val_acc: 0.2385\n",
      "Epoch 3/10\n",
      "2513/2513 [==============================] - 4s 1ms/step - loss: 0.9312 - acc: 0.5639 - val_loss: 0.9006 - val_acc: 0.5040\n",
      "Epoch 4/10\n",
      "2513/2513 [==============================] - 3s 1ms/step - loss: 0.8868 - acc: 0.5897 - val_loss: 1.2041 - val_acc: 0.1145\n",
      "Epoch 5/10\n",
      "2513/2513 [==============================] - 4s 1ms/step - loss: 0.8346 - acc: 0.6084 - val_loss: 1.2080 - val_acc: 0.3084\n",
      "Epoch 6/10\n",
      "2513/2513 [==============================] - 3s 1ms/step - loss: 0.7785 - acc: 0.6610 - val_loss: 0.9258 - val_acc: 0.4913\n",
      "Epoch 7/10\n",
      "2513/2513 [==============================] - 3s 1ms/step - loss: 0.7274 - acc: 0.6809 - val_loss: 1.2959 - val_acc: 0.3116\n",
      "Epoch 8/10\n",
      "2513/2513 [==============================] - 3s 1ms/step - loss: 0.6881 - acc: 0.7087 - val_loss: 1.1811 - val_acc: 0.4102\n",
      "Epoch 9/10\n",
      "2513/2513 [==============================] - 3s 1ms/step - loss: 0.6481 - acc: 0.7278 - val_loss: 1.1021 - val_acc: 0.4913\n",
      "Epoch 10/10\n",
      "2513/2513 [==============================] - 3s 1ms/step - loss: 0.6156 - acc: 0.7398 - val_loss: 1.3367 - val_acc: 0.3418\n",
      "1017/1017 [==============================] - 0s 355us/step\n",
      "1017/1017 [==============================] - 1s 638us/step\n",
      "the training accuracy is 0.6902654874289633\n",
      "the testing accuracy is (array([0.76566125, 0.34939759, 0.18055556]), array([0.88235294, 0.16022099, 0.14772727]), array([0.81987578, 0.21969697, 0.1625    ]), array([748, 181,  88], dtype=int64))\n",
      "the average Training Accuracy of ANN-embedding is 0.598820058586792\n",
      "the averag Validation accuracy of ANN-embedding is [[7.11809120e-01 4.15287412e-01 1.61925382e-01]\n",
      " [8.04475463e-01 1.02124402e-01 2.70247597e-01]\n",
      " [7.55084086e-01 1.56009245e-01 1.92681242e-01]\n",
      " [6.92000000e+02 2.22333333e+02 1.02666667e+02]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "kf= KFold(n_splits=3)\n",
    "#curr_fold= 0\n",
    "alg_accurcy_ANN_embedding =[]\n",
    "alg_testing_accurcy_ANN_embedding =[]\n",
    "\n",
    "for train_idx, test_idx in kf.split(final_data_ANN):\n",
    "    X_train, X_test= final_data_ANN[train_idx], final_data_ANN[test_idx]\n",
    "    y_train, y_test= rationale_labels_ANN[train_idx], rationale_labels_ANN[test_idx]\n",
    "        \n",
    "    model, test_accuracy,report_accuracy = evaluate_ANN_Model(X_train,y_train, X_test, y_test )\n",
    "    print (\"the training accuracy is\",test_accuracy )\n",
    "    alg_accurcy_ANN_embedding.append(test_accuracy)\n",
    "    print (\"the testing accuracy is\",report_accuracy )\n",
    "    alg_testing_accurcy_ANN_embedding.append(report_accuracy)\n",
    "    \n",
    "\n",
    "average_accuracy_ANN_training= np.mean(alg_accurcy_ANN_embedding, axis=0)\n",
    "print(\"the average Training Accuracy of ANN-embedding is\",average_accuracy_ANN_training) \n",
    "\n",
    "average_accuracy_ANN_testing= np.mean(alg_testing_accurcy_ANN_embedding, axis=0)\n",
    "print(\"the averag Validation accuracy of ANN-embedding is\",average_accuracy_ANN_testing)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
