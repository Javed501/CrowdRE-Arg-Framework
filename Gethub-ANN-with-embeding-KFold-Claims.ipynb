{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import keras\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "#nltk.download('popular')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, Flatten,Input \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from numpy import mean, std\n",
    "from sklearn.metrics import classification_report\n",
    "import plotly.graph_objs as go\n",
    "import plotly.plotly as py\n",
    "#import cufflinks\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import plotly.figure_factory as ff\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "from plotly.offline import iplot\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2077\n",
      "neu           772\n",
      "supporting    762\n",
      "attacking     542\n",
      "Name: Claim_Type, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_Text</th>\n",
       "      <th>comment_author</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_upvotes</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>Rationale_Type</th>\n",
       "      <th>Claim_Type</th>\n",
       "      <th>Have_Rationale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>People still do racial checkbox ads?</td>\n",
       "      <td>PubicLouseInDaHouse</td>\n",
       "      <td>dfadazi</td>\n",
       "      <td>-1</td>\n",
       "      <td>60v5za</td>\n",
       "      <td>claim</td>\n",
       "      <td>neu</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What?</td>\n",
       "      <td>Isvara</td>\n",
       "      <td>dfade5q</td>\n",
       "      <td>1</td>\n",
       "      <td>dfadazi</td>\n",
       "      <td>claim</td>\n",
       "      <td>neu</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90s power rangers.</td>\n",
       "      <td>PubicLouseInDaHouse</td>\n",
       "      <td>dfadlgi</td>\n",
       "      <td>1</td>\n",
       "      <td>dfade5q</td>\n",
       "      <td>claim</td>\n",
       "      <td>neu</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://support.google.com/plus/answer/2998354...</td>\n",
       "      <td>sparr</td>\n",
       "      <td>dfa365t</td>\n",
       "      <td>-1</td>\n",
       "      <td>60v5za</td>\n",
       "      <td>claim</td>\n",
       "      <td>supporting</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>This is really cool for people who have friends.</td>\n",
       "      <td>Unidan_nadinU</td>\n",
       "      <td>df9vhad</td>\n",
       "      <td>-1</td>\n",
       "      <td>60v5za</td>\n",
       "      <td>claim</td>\n",
       "      <td>supporting</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_Text       comment_author  \\\n",
       "1               People still do racial checkbox ads?  PubicLouseInDaHouse   \n",
       "2                                              What?               Isvara   \n",
       "3                                 90s power rangers.  PubicLouseInDaHouse   \n",
       "5  https://support.google.com/plus/answer/2998354...                sparr   \n",
       "6  This is really cool for people who have friends.         Unidan_nadinU   \n",
       "\n",
       "  comment_id  comment_upvotes parent_id Rationale_Type  Claim_Type  \\\n",
       "1    dfadazi               -1    60v5za          claim         neu   \n",
       "2    dfade5q                1   dfadazi          claim         neu   \n",
       "3    dfadlgi                1   dfade5q          claim         neu   \n",
       "5    dfa365t               -1    60v5za          claim  supporting   \n",
       "6    df9vhad               -1    60v5za          claim  supporting   \n",
       "\n",
       "  Have_Rationale  \n",
       "1             no  \n",
       "2             no  \n",
       "3             no  \n",
       "5            yes  \n",
       "6            yes  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('google-maps-single-dataset-for-conference.csv',encoding='ISO-8859-1')\n",
    "data= data[pd.notnull(data['Claim_Type'])]\n",
    "print(len(data))\n",
    "\n",
    "data = data.replace(\"neu \", \"neu\")\n",
    "data = data.replace(\"supporting \", \"supporting\")\n",
    "data = data.replace(\"attacking \", \"attacking\")\n",
    "data = data.replace(\"atacking \", \"attacking\")\n",
    "data = data.replace(\"attacknig \", \"attacking\")\n",
    "\n",
    "data= data[data.Rationale_Type=='claim']\n",
    "print(data.Claim_Type.value_counts())\n",
    "#data = data.replace(\"issue \", \"issue\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_plot(index):\n",
    "    example = data[data.index == index][['comment_Text', 'Rationale_Type']].values[0]\n",
    "    if len(example) > 0:\n",
    "        print(example[0])\n",
    "        print('Rationale_Type:', example[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> gender equality in any and all professions.\r\n",
      "\r\n",
      "Hahah uh... what?\r\n",
      "\r\n",
      "EDIT: Also I wasn't the one to downvote you above. No clue what that's about.\n",
      "Rationale_Type: claim\n"
     ]
    }
   ],
   "source": [
    "print_plot(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\<\\>\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z =#+_]')\n",
    "#STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_url(text):\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = strip_html(text) \n",
    "    text = remove_url(text)\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    #text = text.replace('x', '')\n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    #text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "data['comment_Text'] = data['comment_Text'].apply(clean_text)\n",
    "#df['Consumer complaint narrative'] = df['Consumer complaint narrative'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  gender equality in any and all professionshahah uh whatedit also i wasnt the one to downvote you above no clue what thats about\n",
      "Rationale_Type: claim\n"
     ]
    }
   ],
   "source": [
    "print_plot(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neu           772\n",
      "supporting    762\n",
      "attacking     542\n",
      "Name: Claim_Type, dtype: int64\n",
      "['neu' 'supporting' 'attacking']\n"
     ]
    }
   ],
   "source": [
    "print(data.Claim_Type.value_counts())\n",
    "#X= data.comment_Text.values.astype('U')\n",
    "X= data.comment_Text.values.astype('U')\n",
    "y=data.Claim_Type.values.astype('U')\n",
    "rationale_type=data.Claim_Type\n",
    "rationale_type_name= data.Claim_Type.unique()\n",
    "print(rationale_type_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['attacking', 'neu', 'supporting']\n"
     ]
    }
   ],
   "source": [
    "macronum=sorted(set(data['Claim_Type']))\n",
    "print(macronum)\n",
    "macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n",
    "\n",
    "def fun(i):\n",
    "    return macro_to_id[i]\n",
    "\n",
    "data['Claim_Type']=data['Claim_Type'].apply(fun)\n",
    "#print(data['Rationale_Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "rationale_labels = []\n",
    "for idx in data['Claim_Type']:\n",
    "    rationale_labels.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Tokens 5883\n",
      "(2076, 1000) (2076, 3)\n"
     ]
    }
   ],
   "source": [
    "vocab_size= 10000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "tokenizer= Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences_ANN = tokenizer.texts_to_sequences(X)\n",
    "word_index = tokenizer.word_index\n",
    "print('Number of Unique Tokens',len(word_index))\n",
    "final_data_ANN = pad_sequences(sequences_ANN, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "#print(final_data_ANN)\n",
    "rationale_labels_ANN=to_categorical(np.asarray(rationale_labels))\n",
    "print(final_data_ANN.shape, rationale_labels_ANN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size= 10000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "batch_size= 12\n",
    "epochs= 10\n",
    "\n",
    "test_accuracy =[]\n",
    "report_accuracy= []\n",
    "\n",
    "def evaluate_ANN_Model(trainX,trainy, testX, testy):\n",
    "    \n",
    "    \n",
    "    embeddings_index = {}\n",
    "    f = open('glove.6B.100d.txt',encoding='utf8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))\n",
    "    \n",
    "    \n",
    "    EMBEDDING_DIM=100\n",
    "    MAX_SEQUENCE_LENGTH = 1000\n",
    "    #embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    embedding_matrix = zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "    \n",
    "    \n",
    "    smote = SMOTE('minority')\n",
    "    X_sm_embedding, y_sm_embedding = smote.fit_sample(trainX, trainy)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(embedding_layer) \n",
    "    model.add(Flatten())\n",
    "    #input layer\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.35))\n",
    "    \n",
    "    model.add(Dense(3))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer= 'SGD', metrics=['accuracy'])  \n",
    "    \n",
    "    #class_weight= class_weight.compute_class_weight('balanced', np.unique(rationale_type),rationale_type)\n",
    "    \n",
    "   \n",
    "    #model.compile(loss='categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
    "     \n",
    "    model.fit(X_sm_embedding,y_sm_embedding, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.2)\n",
    "    \n",
    "\n",
    "    #evaluate model\n",
    "    score= model.evaluate(testX, testy, batch_size=batch_size, verbose=1)\n",
    "    test_accuracy= score[1]\n",
    "    #predict model accuracy on testing data\n",
    "    pred = model.predict(testX, batch_size=12, verbose=1)\n",
    "    predicted = np.argmax(pred, axis=1)\n",
    "    report_accuracy = precision_recall_fscore_support(np.argmax(testy, axis=1), predicted)\n",
    "    return model, test_accuracy, report_accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors in Glove 6B 100d.\n",
      "Train on 1270 samples, validate on 318 samples\n",
      "Epoch 1/10\n",
      "1270/1270 [==============================] - 8s 6ms/step - loss: 1.0944 - acc: 0.3780 - val_loss: 1.2011 - val_acc: 0.1824\n",
      "Epoch 2/10\n",
      "1270/1270 [==============================] - 3s 2ms/step - loss: 1.0616 - acc: 0.4197 - val_loss: 1.2004 - val_acc: 0.1918\n",
      "Epoch 3/10\n",
      "1270/1270 [==============================] - 2s 2ms/step - loss: 1.0389 - acc: 0.4504 - val_loss: 1.0981 - val_acc: 0.3836\n",
      "Epoch 4/10\n",
      "1270/1270 [==============================] - 2s 2ms/step - loss: 1.0030 - acc: 0.4992 - val_loss: 1.1238 - val_acc: 0.28620s - loss: 1.0079 - acc: 0.5 - ETA: 0s - loss: 1.0045\n",
      "Epoch 5/10\n",
      "1270/1270 [==============================] - 2s 2ms/step - loss: 0.9553 - acc: 0.5559 - val_loss: 1.3349 - val_acc: 0.1918\n",
      "Epoch 6/10\n",
      "1270/1270 [==============================] - 2s 2ms/step - loss: 0.9146 - acc: 0.5756 - val_loss: 1.1367 - val_acc: 0.3459\n",
      "Epoch 7/10\n",
      "1270/1270 [==============================] - 2s 2ms/step - loss: 0.8602 - acc: 0.6000 - val_loss: 1.2692 - val_acc: 0.3428\n",
      "Epoch 8/10\n",
      "1270/1270 [==============================] - 2s 2ms/step - loss: 0.8072 - acc: 0.6402 - val_loss: 1.2298 - val_acc: 0.3333\n",
      "Epoch 9/10\n",
      "1270/1270 [==============================] - 2s 1ms/step - loss: 0.7436 - acc: 0.6937 - val_loss: 1.2776 - val_acc: 0.3019\n",
      "Epoch 10/10\n",
      "1270/1270 [==============================] - 2s 2ms/step - loss: 0.6873 - acc: 0.7189 - val_loss: 1.3115 - val_acc: 0.3113\n",
      "692/692 [==============================] - 0s 411us/step\n",
      "692/692 [==============================] - 1s 1ms/step\n",
      "the training accuracy is 0.42630057575213426\n",
      "the testing accuracy is (array([0.31538462, 0.42083333, 0.47515528]), array([0.21925134, 0.4741784 , 0.5239726 ]), array([0.25867508, 0.44591611, 0.49837134]), array([187, 213, 292], dtype=int64))\n",
      "Total 400000 word vectors in Glove 6B 100d.\n",
      "Train on 1251 samples, validate on 313 samples\n",
      "Epoch 1/10\n",
      "1251/1251 [==============================] - 9s 7ms/step - loss: 1.0829 - acc: 0.3997 - val_loss: 1.1183 - val_acc: 0.1182\n",
      "Epoch 2/10\n",
      "1251/1251 [==============================] - 2s 2ms/step - loss: 1.0480 - acc: 0.4293 - val_loss: 1.1258 - val_acc: 0.16930451 -\n",
      "Epoch 3/10\n",
      "1251/1251 [==============================] - 2s 2ms/step - loss: 1.0198 - acc: 0.4580 - val_loss: 1.3301 - val_acc: 0.1118\n",
      "Epoch 4/10\n",
      "1251/1251 [==============================] - 2s 2ms/step - loss: 0.9919 - acc: 0.4996 - val_loss: 1.3446 - val_acc: 0.1310\n",
      "Epoch 5/10\n",
      "1251/1251 [==============================] - 2s 2ms/step - loss: 0.9622 - acc: 0.5188 - val_loss: 1.1363 - val_acc: 0.3355\n",
      "Epoch 6/10\n",
      "1251/1251 [==============================] - 2s 2ms/step - loss: 0.9234 - acc: 0.5364 - val_loss: 1.3486 - val_acc: 0.1757\n",
      "Epoch 7/10\n",
      "1251/1251 [==============================] - 2s 2ms/step - loss: 0.8757 - acc: 0.5707 - val_loss: 1.0917 - val_acc: 0.4089\n",
      "Epoch 8/10\n",
      "1251/1251 [==============================] - 2s 2ms/step - loss: 0.8456 - acc: 0.6043 - val_loss: 1.2300 - val_acc: 0.2652\n",
      "Epoch 9/10\n",
      "1251/1251 [==============================] - 2s 2ms/step - loss: 0.7723 - acc: 0.6491 - val_loss: 1.1279 - val_acc: 0.4728\n",
      "Epoch 10/10\n",
      "1251/1251 [==============================] - 2s 2ms/step - loss: 0.7432 - acc: 0.6579 - val_loss: 1.3379 - val_acc: 0.3291\n",
      "692/692 [==============================] - 0s 502us/step\n",
      "692/692 [==============================] - 1s 1ms/step\n",
      "the training accuracy is 0.4277456661610934\n",
      "the testing accuracy is (array([0.38235294, 0.544     , 0.40835267]), array([0.26666667, 0.27755102, 0.6984127 ]), array([0.3141994 , 0.36756757, 0.51537335]), array([195, 245, 252], dtype=int64))\n",
      "Total 400000 word vectors in Glove 6B 100d.\n",
      "Train on 1236 samples, validate on 310 samples\n",
      "Epoch 1/10\n",
      "1236/1236 [==============================] - 6s 5ms/step - loss: 1.0997 - acc: 0.3827 - val_loss: 1.1415 - val_acc: 0.1871\n",
      "Epoch 2/10\n",
      "1236/1236 [==============================] - 2s 2ms/step - loss: 1.0873 - acc: 0.4021 - val_loss: 1.1415 - val_acc: 0.2065\n",
      "Epoch 3/10\n",
      "1236/1236 [==============================] - 3s 2ms/step - loss: 1.0690 - acc: 0.4215 - val_loss: 1.1418 - val_acc: 0.2290\n",
      "Epoch 4/10\n",
      "1236/1236 [==============================] - 2s 2ms/step - loss: 1.0533 - acc: 0.4531 - val_loss: 1.1400 - val_acc: 0.2839\n",
      "Epoch 5/10\n",
      "1236/1236 [==============================] - 3s 2ms/step - loss: 1.0131 - acc: 0.4871 - val_loss: 1.1644 - val_acc: 0.2355\n",
      "Epoch 6/10\n",
      "1236/1236 [==============================] - 3s 2ms/step - loss: 0.9545 - acc: 0.5291 - val_loss: 1.1789 - val_acc: 0.2419\n",
      "Epoch 7/10\n",
      "1236/1236 [==============================] - 3s 2ms/step - loss: 0.9049 - acc: 0.5461 - val_loss: 1.0438 - val_acc: 0.4871\n",
      "Epoch 8/10\n",
      "1236/1236 [==============================] - 2s 2ms/step - loss: 0.8679 - acc: 0.6036 - val_loss: 1.1409 - val_acc: 0.2903\n",
      "Epoch 9/10\n",
      "1236/1236 [==============================] - 2s 2ms/step - loss: 0.8231 - acc: 0.6222 - val_loss: 1.2289 - val_acc: 0.3484\n",
      "Epoch 10/10\n",
      "1236/1236 [==============================] - 2s 2ms/step - loss: 0.7677 - acc: 0.6489 - val_loss: 1.2576 - val_acc: 0.3129\n",
      "692/692 [==============================] - 0s 451us/step\n",
      "692/692 [==============================] - 1s 1ms/step\n",
      "the training accuracy is 0.4855491318713034\n",
      "the testing accuracy is (array([0.43925234, 0.59016393, 0.42521994]), array([0.29375   , 0.45859873, 0.66513761]), array([0.35205993, 0.51612903, 0.51878354]), array([160, 314, 218], dtype=int64))\n",
      "the average Training Accuracy of ANN-embedding is 0.4465317912615103\n",
      "the averag Validation accuracy of ANN-embedding is [[  0.37899663   0.51833242   0.43624263]\n",
      " [  0.25988933   0.40344272   0.62917431]\n",
      " [  0.30831147   0.44320424   0.51084274]\n",
      " [180.66666667 257.33333333 254.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Kfold ANN with TFIDF\n",
    "\n",
    "kf= KFold(n_splits=3)\n",
    "#curr_fold= 0\n",
    "alg_accurcy_ANN_embedding =[]\n",
    "alg_testing_accurcy_ANN_embedding =[]\n",
    "\n",
    "for train_idx, test_idx in kf.split(final_data_ANN):\n",
    "    X_train, X_test= final_data_ANN[train_idx], final_data_ANN[test_idx]\n",
    "    y_train, y_test= rationale_labels_ANN[train_idx], rationale_labels_ANN[test_idx]\n",
    "        \n",
    "    model, test_accuracy,report_accuracy = evaluate_ANN_Model(X_train,y_train, X_test, y_test )\n",
    "    print (\"the training accuracy is\",test_accuracy )\n",
    "    alg_accurcy_ANN_embedding.append(test_accuracy)\n",
    "    print (\"the testing accuracy is\",report_accuracy )\n",
    "    alg_testing_accurcy_ANN_embedding.append(report_accuracy)\n",
    "    \n",
    "\n",
    "average_accuracy_ANN_training= np.mean(alg_accurcy_ANN_embedding, axis=0)\n",
    "print(\"the average Training Accuracy of ANN-embedding is\",average_accuracy_ANN_training) \n",
    "\n",
    "average_accuracy_ANN_testing= np.mean(alg_testing_accurcy_ANN_embedding, axis=0)\n",
    "print(\"the averag Validation accuracy of ANN-embedding is\",average_accuracy_ANN_testing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
